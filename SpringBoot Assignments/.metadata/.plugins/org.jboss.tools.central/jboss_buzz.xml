<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to trigger jobs manually in Packit</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/08/how-trigger-jobs-manually-packit" /><author><name>Jakub Stejskal, David Kornel</name></author><id>13909259-cf88-4513-a5a4-851cb12e8e99</id><updated>2023-09-08T07:00:00Z</updated><published>2023-09-08T07:00:00Z</published><summary type="html">&lt;p&gt;Packit is an open source project aiming to ease your project's integration with Fedora &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/rhel/centos-and-rhel"&gt;CentOS Stream&lt;/a&gt;, and other distributions. Packit is mostly used by projects that build RPM packages. We won't go through the onboarding process that was already described in a &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration#"&gt;previous article&lt;/a&gt;, but we would like to introduce you to new features that were recently promoted into production.&lt;/p&gt; &lt;h2&gt;Testing Farm execution&lt;/h2&gt; &lt;p&gt;From Packit, you can easily trigger the tests on Testing Farm even without building the RPMs. This is very handy for projects that basically don't build RPMs but want to use these two services for verifying the code. As a good example, we can refer to the &lt;a href="https://strimzi.io/"&gt;Strimzi project&lt;/a&gt; where users consume &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images.&lt;/p&gt; &lt;p&gt;In such cases, the users want to trigger the tests, verify the code and see some output. This option is available from the beginning. Users can easily define when to execute the tests for every pull request, commit, or release. That sounds pretty cool; however, when you have complex tests (5+ hours per test run) as we have in Strimzi, you probably don't want to trigger all tests for each commit. So, how can the users achieve that?&lt;/p&gt; &lt;h2&gt;Manual trigger&lt;/h2&gt; &lt;p&gt;We introduced a new configuration option &lt;code&gt;manual_trigger&lt;/code&gt; to enable triggering Packit jobs only manually. With this new configuration of Packit jobs, users can easily enable the manual trigger of a job, and this job is not automatically triggered when, for example, a new commit arrives to pull a request.&lt;/p&gt; &lt;p&gt;Users just need to specify &lt;code&gt;manual_trigger&lt;/code&gt; in the test's job description. Only boolean values are allowed with the default configuration set to &lt;code&gt;False&lt;/code&gt;. Examples of manual trigger configurations can be found in the &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/.packit.yaml"&gt;YAML file&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; ... - job: tests trigger: pull_request identifier: "regression-operators" targets: - centos-stream-9-x86_64 - centos-stream-9-aarch64 skip_build: true manual_trigger: true labels: - regression - operators - regression-operators - ro tf_extra_params: test: fmf: name: regression-operators ... &lt;/pre&gt; &lt;p&gt;This new configuration allows users to onboard a new flow when a pull request is opened. For example, in draft mode, users push new commits and fixes, and when they are about to finish the pull request, they can easily type &lt;code&gt;/packit test&lt;/code&gt; as a pull request comment, and all jobs defined in &lt;code&gt;packit.yaml&lt;/code&gt; for the pull request are triggered.&lt;/p&gt; &lt;h2&gt;Labeling and identifying&lt;/h2&gt; &lt;p&gt;The solution just described is very easy to use; however, there might be use cases where the users don't want to trigger all the jobs. For example, when you have 10 jobs defined with different test scopes, you probably don't want to trigger acceptance and regression tests at the same time as acceptance could be a subset of regression.&lt;/p&gt; &lt;p&gt;Users now have two options for triggering a specific job. The first one is to trigger the job based on the identifier. When the user specifies &lt;code&gt;identifier: test-1&lt;/code&gt; in the job configuration, the Packit comment command for execution of the tests will look like this &lt;code&gt;/packit test –identifier test1&lt;/code&gt;. That command will execute jobs with this specific identifier, nothing else (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-22_at_11.27.52.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-22_at_11.27.52.png?itok=ofWtbNJ4" width="600" height="506" alt="Triggering jobs based on a specific identifier." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Packit manual trigger 1.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;What if we want to execute more than one job? Users can use multiple identifiers in a comma-separated list, but it might be a little bit annoying to specify long identifiers every time. To add a better user experience, we've introduced &lt;code&gt;labels&lt;/code&gt; configuration that could group together multiple jobs. Command &lt;code&gt;/packit test –labels upgrade,regression&lt;/code&gt; will trigger all jobs that contain &lt;code&gt;upgrade&lt;/code&gt; or &lt;code&gt;regression&lt;/code&gt; in the list of labels in the job configuration.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-22_at_11.30.38.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-22_at_11.30.38.png?itok=jBRlKw9l" width="600" height="506" alt="Triggering a group of jobs via the label configuration." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Packit manual trigger 2.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you hesitated with onboarding to Packit due to the limitation of missing manual triggering of the jobs and missing labeling, you can start with onboarding now! As we already mentioned, Packit is an &lt;a href="https://devconfcz2023.sched.com/event/1MYme/become-an-open-source-service?linkback=grid"&gt;open source service&lt;/a&gt; and these improvements were done as contributions from outside of the Packit team. Everyone can contribute, so if you are missing some features, feel free to open a pull request.&lt;/p&gt; &lt;p&gt;To see more information about newly added options, check out the &lt;a href="https://packit.dev/docs/testing-farm/"&gt;documentation&lt;/a&gt;. If you are new to Packit, you can also watch talks from the Packit team from &lt;a href="https://devconfcz2023.sched.com/event/1MYlL/packit-rpm-integration-all-in-one?linkback=grid"&gt;DevConf 2023&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=e2aCilMy-5U"&gt;DevConf Mini 2023&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/08/how-trigger-jobs-manually-packit" title="How to trigger jobs manually in Packit"&gt;How to trigger jobs manually in Packit&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jakub Stejskal, David Kornel</dc:creator><dc:date>2023-09-08T07:00:00Z</dc:date></entry><entry><title>Drop git pull for fetch and rebase</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/07/drop-git-pull-fetch-and-rebase" /><author><name>Yftach Herzog</name></author><id>f06cdd57-aa99-482f-834b-86238f0e8672</id><updated>2023-09-07T07:00:00Z</updated><published>2023-09-07T07:00:00Z</published><summary type="html">&lt;p&gt;I would like to explain why the &lt;code&gt;git pull&lt;/code&gt; command is not to be used lightly and to question whether it is ever needed. The &lt;code&gt;git pull&lt;/code&gt; command may look harmless, but it is used in ways that often leave a fair amount of mess. I will discuss safer alternatives. This article is for beginner to intermediate Git users looking to extend their skills in using pull requests and merge requests when collaborating on a project.&lt;/p&gt; &lt;h2&gt;Alternatives to git pull&lt;/h2&gt; &lt;p&gt;This section provides a condensed version of an approach for contributing to a software project without using &lt;code&gt;git pull&lt;/code&gt;. I will go into more detail later.&lt;/p&gt; &lt;p&gt;Configure two remotes on your local repository so that you have &lt;code&gt;origin&lt;/code&gt; pointing to your fork and &lt;code&gt;upstream&lt;/code&gt; pointing to the repository you’re contributing to as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git clone forked-repo-url → cd repo-name → git remote add upstream upstream-repo-url → git remote -v origin forked-repo-url (fetch) origin forked-repo-url (push) upstream upstream-repo-url (fetch) upstream upstream-repo-url (push) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Forget about your fork’s main branch. There is no reason to keep your main branch in sync with the upstream repository’s main branch. It’s a maintenance burden that serves no purpose. You should only maintain branches that are part of an ongoing PR work. The rest can be deleted.&lt;/p&gt; &lt;h3&gt;Starting pull request work&lt;/h3&gt; &lt;p&gt;Create your PR branch directly from the upstream repository’s branch it should be merged to (typically main):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream → git checkout -b my-pr-branch upstream/main → git add new-file some-other-file → git commit → git push origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Syncing with ongoing work&lt;/h3&gt; &lt;p&gt;Use &lt;code&gt;rebase&lt;/code&gt; when you need to have your PR branch synchronized with changes on the target branch (address conflicts as needed):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream → git checkout my-pr-branch → git rebase upstream/main&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Don’t add unnecessary commits&lt;/h3&gt; &lt;p&gt;Don’t create new commits throughout the PR progression. Try limiting your PR to a single commit and add later changes by amending the original commit. Use force push to update the remote branch:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add changed-file another-file → git commit --amend → git push --force origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In other words, use &lt;code&gt;git commit&lt;/code&gt; (without &lt;code&gt;--amend&lt;/code&gt;) only for the first time you create the PR’s commit(s). Later on, only use &lt;code&gt;git commit --amend&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Typical change workflow&lt;/h2&gt; &lt;p&gt;Let’s assume this is a scenario in which developers contribute code to a repository. We’ll call it the upstream repository, to which they don’t necessarily have write access.&lt;/p&gt; &lt;p&gt;To contribute code, developers will do the following:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Fork the upstream repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Clone their forked repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a feature branch out of the main branch of their forked repository (assuming its name is main, but it can be any other name).&lt;/li&gt; &lt;li aria-level="1"&gt;Introduce the code changes locally, commit, and push them to the newly-created feature branch on the forked repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a pull request on GitHub (or a merge request on GitLab).&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;At this point, developers will ask their peers to review their code, address peers' comments, and push the changes to their forked repository in order to have their PR approved and merged.&lt;/p&gt; &lt;p&gt;But what happens when the upstream repository progresses? There are times when we need our feature branch to include the latest changes from the target branch (the upstream repository’s main branch). This might be because of conflicts between our work and the upstream repository, or maybe we have automated tests that have to run on an up-to-date feature branch to verify that we didn't introduce regressions.&lt;/p&gt; &lt;p&gt;When we need to get in sync with the upstream repository, a simpler scenario is where we want to create another PR to the same repository, but we don’t have the latest progress made on the upstream repository.&lt;/p&gt; &lt;h2&gt;Using git pull is risky&lt;/h2&gt; &lt;p&gt;How do I get all that upstream progress to my PR branch? At this point, one might say &lt;code&gt;git pull&lt;/code&gt; must be the opposite of &lt;code&gt;git push&lt;/code&gt;, so let’s use it to update my stuff with the upstream changes.&lt;/p&gt; &lt;p&gt;But &lt;code&gt;git pull&lt;/code&gt; is the opposite of &lt;code&gt;git push&lt;/code&gt; only in very specific cases. That is, when the local checked out branch can be fast-forwarded to the state of the branch being pulled.&lt;/p&gt; &lt;p&gt;When we want to push commits to an existing remote branch, &lt;code&gt;git push&lt;/code&gt; will only go through if the remote branch did not diverge from our local branch. It only contains commits that exist on the local branch. If the remote branch contains commits not on the local branch, &lt;code&gt;git push&lt;/code&gt; will fail.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git push origin HEAD To /tmp/tmp.B2Ljc86u9L ! [rejected] HEAD -&gt; foo (non-fast-forward) error: failed to push some refs to '/tmp/tmp.B2Ljc86u9L' hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is not the same for &lt;code&gt;git pull&lt;/code&gt;. The &lt;code&gt;git pull&lt;/code&gt; command performs &lt;code&gt;git fetch&lt;/code&gt; and then &lt;code&gt;git merge&lt;/code&gt; (this is configurable, but those are the typical defaults).&lt;/p&gt; &lt;p&gt;The &lt;code&gt;git fetch&lt;/code&gt; command will update the remote-tracking branches (local branches mirroring remote branches), which is harmless.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;git merge&lt;/code&gt; command will merge the changes on the remote-tracking branch to the local branch.&lt;/p&gt; &lt;p&gt;This has some drawbacks:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;If those changes cannot be fast-forwarded, it means a merge-commit will be created on the local branch. &lt;ul&gt;&lt;li aria-level="2"&gt;If that local branch is our main branch, this is probably not what we want.&lt;/li&gt; &lt;li aria-level="2"&gt;If that local branch is a PR branch, it means that our PR will now include a merge-commit, which is confusing for reviewers and makes our history look ugly.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Which remote branch is actually going to be merged into our local branch? We can control that, but we cannot assume git will necessarily be smart about picking the right one.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;All in all, using &lt;code&gt;git pull&lt;/code&gt;, puts us at risk of turning our PR branch (and the upstream branch if the changes are merged) into a merge-commits spaghetti or even merging changes from unexpected remote branches into our PR branch.&lt;/p&gt; &lt;h2&gt;Take control&lt;/h2&gt; &lt;p&gt;Git is powerful. Which means harmful actions can easily happen. We should be even more cautious with using git shortcuts embedded into our IDEs, graphical git utilities, and nice-looking buttons on GitHub that are supposed to solve our issues with a click.&lt;/p&gt; &lt;p&gt;At least some of those shortcuts can't read our minds yet. They will not do what we expect them to do, especially if we don’t know what we want them to do. They surely won’t clean up the mess they made.&lt;/p&gt; &lt;h3&gt;Do we need so many branches?&lt;/h3&gt; &lt;p&gt;When we navigate to our fork in GitHub, it usually warns us that our main branch is many commits behind the upstream branch (Figure 1). This makes us think that our main branch should be in sync with the upstream branch.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/gazillion-commits-behind.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/gazillion-commits-behind.png?itok=veFTgIEF" width="600" height="47" alt="A screenshot of a GitHub warning for a branch that is many commits behind the upstream branch." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: GitHub warns that our branch is many commits behind the target branch.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But does this branch really serve any purpose?&lt;/p&gt; &lt;p&gt;Let’s assume our fork exists only as a means to contribute code to an upstream repository rather than to develop a spin-off, which is usually the case.&lt;br /&gt; I would argue that our fork’s main branch should not be a part of our contribution workflow. This is because a PR is a proposal for merging a feature branch to an upstream branch. Our fork’s main branch has nothing to do with that.&lt;/p&gt; &lt;p&gt;In other words, we start our PR branches out of the target upstream branch (e.g., the upstream main branch, not our fork’s main branch). In case we need to get our PR branch up-to-speed with current changes, it’s the target upstream branch we need to sync with, not our fork’s main branch.&lt;/p&gt; &lt;p&gt;Synchronizing our fork’s main branch with its upstream counterpart serves no purpose. We don’t need it, and performing pointless tasks is just another opportunity to introduce mistakes and mess up our environments.&lt;/p&gt; &lt;h3&gt;Starting at the right point&lt;/h3&gt; &lt;p&gt;So, how do we start a new PR branch from the tip of our upstream branch? Simple, we fetch our remote-tracking branches for the upstream repository. To do that, we first need to have a remote defined for the upstream repository.&lt;/p&gt; &lt;p&gt;Let’s assume we created our local repository by cloning our fork using the default options. Something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git clone forked-repo-url&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, we should now have a remote called &lt;code&gt;origin&lt;/code&gt; pointing to the forked repo defined under the local repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ cd repo-name → git remote -v origin forked-repo-url (fetch) origin forked-repo-url (push)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will allow us to push changes to our fork.&lt;/p&gt; &lt;p&gt;In order to get the latest changes from the upstream repository, we need to add a remote for that repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git remote add upstream upstream-repo-url → git remote -v origin forked-repo-url (fetch) origin forked-repo-url (push) upstream upstream-repo-url (fetch) upstream upstream-repo-url (push)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We now have the remote defined. To synchronize the remote-tracking branches, we need to fetch:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With that, we create a remote-tracking branch called &lt;code&gt;upstream/main&lt;/code&gt; (assuming that’s the branch for which we want to create a PR). This is a local branch containing the content of the main branch on the upstream repository at the time we last fetched upstream.&lt;/p&gt; &lt;p&gt;We can list our remote-tracking branches:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git branch --remote origin/HEAD -&gt; origin/main upstream/main&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create our feature branch at the current state of &lt;code&gt;upstream/main&lt;/code&gt;, we use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git checkout -b my-pr-branch upstream/main&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a new branch called &lt;code&gt;my-pr-branch&lt;/code&gt;at the commit &lt;code&gt;upstream/main&lt;/code&gt; points to and switches to the newly-created branch.&lt;/p&gt; &lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;About PR progression&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;We should now make our changes, &lt;code&gt;git add&lt;/code&gt; them, commit, and push them to origin:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add changed-file another-file → git commit → git push origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will push our changes to &lt;code&gt;origin&lt;/code&gt; (the remote pointing to our fork) into a branch with the same name as our local branch (the &lt;code&gt;HEAD&lt;/code&gt; keyword points to the latest commit on the currently checked-out branch).&lt;/p&gt; &lt;p&gt;We will now use the GitHub UI to create a PR. We will make sure that the source branch of the PR is the newly-created feature branch on our fork, while the target branch is the branch to which we want to propose changes (in this case, the main branch of the upstream repository).&lt;/p&gt; &lt;p&gt;Our diligent peers will thoroughly review our work and point us to some issues requiring our attention.&lt;/p&gt; &lt;p&gt;We will then fix those issues locally and &lt;code&gt;git add&lt;/code&gt; them, but we will not include them on a new commit. We will instead use them to amend the original commit and then force-push them to the same branch:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add changed-file another-file some-other-file → git commit --amend → git push --force origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason for amending the commit rather than creating a new one is so that our PR’s commits will ultimately represent the progression of the code we propose to introduce to the upstream branch rather than representing the progression of the PR work.&lt;/p&gt; &lt;p&gt;In other words, if our PR intends to fix the fairy dust dispenser, then we want it to include a commit with title “fix the fairy dust dispenser” which will contain all changes required for fixing the fairy dust dispenser, rather than 3 commits titled “fix the fairy dust dispenser”, “removing prints”, “addressing comments”.&lt;/p&gt; &lt;p&gt;By amending the commit, we’re diverging from the remote branch. We created a new commit instead of the one we already pushed. So Git will not allow us to push to it. The remote branch contains our original commit, which our local branch doesn't have anymore.&lt;/p&gt; &lt;p&gt;At this point, it will even suggest that we use git pull to fix it (see the previous rejection message). Don’t use git pull for that. It will not fix our issue and will create others instead. Don’t use git pull at all.&lt;/p&gt; &lt;p&gt;We now know a few things about Git, and we have some confidence in what we’re doing. So we force-push instead, telling Git that we want to replace the content of the remote branch with the content of the local branch. That’s the reason for using &lt;code&gt;push --force&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Fetch and rebase to the rescue&lt;/h3&gt; &lt;p&gt;It might also be that while we were busy waiting for reviews, some other changes merged to the target branch on the upstream repository. In this case, we might need to get in sync with that target branch.&lt;/p&gt; &lt;p&gt;Will it be &lt;code&gt;git pull&lt;/code&gt; to the rescue? No. By default, &lt;code&gt;git pull&lt;/code&gt; will create a merge commit on our feature branch, merging the work done on the target branch since we started working on it. This will make reviewers’ lives harder and, if merged, will not look nice on the target branch history (e.g., how would it look if we try to revert this PR for some reason at a later stage?).&lt;/p&gt; &lt;p&gt;Or will it be &lt;code&gt;git fetch&lt;/code&gt; and &lt;code&gt;git rebase&lt;/code&gt; to the rescue? Indeed!&lt;/p&gt; &lt;p&gt;To overcome this, we must replay our changes on top of the latest changes on the target branch. To do that, we need to synchronize our remote-tracking branch with the upstream repository and rebase our PR branch on top of it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream → git checkout my-pr-branch → git rebase upstream/main Successfully rebased and updated refs/heads/my-pr-branch.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;git rebase&lt;/code&gt; command will find the commit that is the common ancestor of our PR branch and the target branch. That should be the commit from which we started our work. It will take all the commits on our PR branch introduced after that point and replay them on top of the target branch.&lt;/p&gt; &lt;p&gt;Suppose that we started our work when the target branch's Git history (git log) looked something like this (newest commits first):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On our PR branch, we created a commit for the content we want to deliver, so its history looks something like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;our pr commit commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our busy colleagues did some work in the meantime, and the target branch now looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;yet more work done while our pr was in review some work done while our pr was in review commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If we fetch the upstream repo, we now have the remote-tracking branch &lt;code&gt;upstream/main&lt;/code&gt; containing this history.&lt;/p&gt; &lt;p&gt;If we checkout our PR branch and rebase on top of &lt;code&gt;upstream/main&lt;/code&gt;, Git will:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Find the newest commit existing on both branches, which is the one named "commit happened just before…".&lt;/li&gt; &lt;li aria-level="1"&gt;Take all commits on our branch that happened after that point. In this case, it’s the one named "our pr commit".&lt;/li&gt; &lt;li aria-level="1"&gt;Reset our PR branch to the current state of the remote-tracking branch.&lt;/li&gt; &lt;li aria-level="1"&gt;Replay "our pr commit" on top of that.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The result:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;our pr commit yet more work done while our pr was on review some work done while our pr was on review commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replaying our changes on top of the latest changes means that it will create new commit(s) with the same changes from our original commit(s). Namely, they will look the same in terms of the changes they made, but they will have different commit hash because the starting point for the changes is different.&lt;/p&gt; &lt;h3&gt;Rebase looks simple enough&lt;/h3&gt; &lt;p&gt;Rebasing replays the original changes we made on top of the current state of the branch on which we rebase upon. What happens if the starting point of the content we changed is not the same anymore? In other words, what happens if the lines that we changed in the PR's commit(s) were also changed by the commits that were added to the target branch in the meantime?&lt;/p&gt; &lt;p&gt;The answer is: conflicts.&lt;/p&gt; &lt;p&gt;In the following example, we’re trying to rebase branch &lt;code&gt;foobar&lt;/code&gt; with a commit message "add bar" on top of the &lt;code&gt;upstream/main&lt;/code&gt; branch having at its tip a commit called "add baz", which does not exist on the PR branch. The two commits are changing the same line on a file called foo.&lt;/p&gt; &lt;p&gt;In this case, instead of telling us that our branch was successfully rebased, Git will let us know which files it failed to process:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Auto-merging foo CONFLICT (content): Merge conflict in foo error: could not apply 7a384ae... add bar Resolve all conflicts manually, mark them as resolved with "git add/rm &lt;conflicted_files&gt;", then run "git rebase --continue". You can instead skip this commit: run "git rebase --skip". To abort and get back to the state before "git rebase", run "git rebase --abort". Could not apply 7a384ae... add bar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It will also add markers within the failed files denoting the conflict(s):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ cat foo &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD baz ======= bar &gt;&gt;&gt;&gt;&gt;&gt;&gt; 7a384ae (add bar)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It could be a bit confusing, but everything between the &lt;&lt;&lt; markers and the === markers is the content coming from the branch on top of which we’re trying to rebase (the target branch), while the stuff between the === markers and the &gt;&gt;&gt; markers is the content coming from the branch we try to place at the top (the changes coming from our PR branch).&lt;/p&gt; &lt;p&gt;What we now need to do is to decide what should be the correct content for each conflict and then delete all markers. In our case, we’re going to delete the markers and add the conflicting statements under the same line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ cat foo baz bar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If we try to rebase multiple commits, there may be conflicts on each and every commit, and we’d need to decide what should have been the content of each conflicting line for each such conflict at the point in which each commit is applied.&lt;/p&gt; &lt;p&gt;Sounds complicated. Which is yet another reason to make small PRs including only a single commit (easier code review is another reason).&lt;/p&gt; &lt;p&gt;Once we resolve all conflicts, we need to &lt;code&gt;git add&lt;/code&gt; all files that had conflicts, and run &lt;code&gt;git rebase --continue&lt;/code&gt;. A common mistake is committing the changes instead.&lt;/p&gt; &lt;p&gt;Do not use &lt;code&gt;git commit&lt;/code&gt; during a rebase process, as we’re not trying to add a new commit, just to fix conflicts on existing commits:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add foo → git rebase --continue ... Successfully rebased and updated refs/heads/foobar.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can force-push the changes and later repeat the same steps if you need to attend to more issues until your code is ready to be merged.&lt;/p&gt; &lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Tips to track your progress&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;It is easy to lose track of your current step, so follow these tips.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Use &lt;code&gt;git status&lt;/code&gt; to see your staged and unstaged changes and the branch you have checked out.&lt;/li&gt; &lt;li aria-level="1"&gt;Use &lt;code&gt;git log&lt;/code&gt; to convince yourself that your Git history makes sense. Each commit line includes all branches pointing to this commit. After rebasing on top of a branch, you should see your commit(s) at the top, and the target branch just underneath. If that is not the case, then we need to figure out what we did wrong.&lt;br /&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;In this example, we’re on branch foobar, and we rebased the “add bar” commit on top of &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;upstream/main&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;pre&gt; &lt;code&gt;af2e399 (HEAD -&gt; foobar) add bar 5ba3bf7 (upstream/main) add baz 3b3b1bb add foo&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;At any step before the rebase process is done, you can abort it and revert to the stage before the process started with &lt;code&gt;git rebase --abort&lt;/code&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;If you realize you made a mistake, there are still a few ways to go back. One such option is to override your local branch with the last version you pushed to the remote (losing all local progress): &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch origin → git checkout my-pr-branch → git reset --hard origin/my-pr-branch&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Use &lt;code&gt;git show&lt;/code&gt; before you push your changes to the remote to show the content of your last commit in order to convince yourself it makes sense. Until you push your changes, you still have the remote branch as backup.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Contrary to this somewhat opinionated text, I honestly believe that everyone should do what works for them. In my opinion, using &lt;code&gt;git pull&lt;/code&gt; is problematic, and I have explained why and offered alternatives. What is important to keep in mind is that while Git is powerful, it is not forgiving. You can use &lt;code&gt;git reflog&lt;/code&gt; to undo many mistakes, but it’s not easy to use. For this reason, when we do something, we need to know the expected result, such as which branch or remote will be affected and what is changing. Hopefully, you will also know how the chosen steps will take you there.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/07/drop-git-pull-fetch-and-rebase" title="Drop git pull for fetch and rebase"&gt;Drop git pull for fetch and rebase&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yftach Herzog</dc:creator><dc:date>2023-09-07T07:00:00Z</dc:date></entry><entry><title>Quarkus extensions give Java dependencies superpowers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/06/introduction-quarkus-extensions-java-dependencies" /><author><name>Kevin Dubois</name></author><id>1865d737-c8ff-415b-9ed7-1b4b8fc7678a</id><updated>2023-09-06T07:00:00Z</updated><published>2023-09-06T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; extensions are one of Quarkus' best hidden-in-plain-sight features. Read on to learn how Quarkus extensions give &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; superpowers and how you can get started with them.  &lt;/p&gt; &lt;h2&gt;What are Quarkus extensions?&lt;/h2&gt; &lt;p&gt;Quarkus extensions are essentially adapter layers for Java-based libraries or technologies that enhance your application.&lt;/p&gt; &lt;p&gt;However, the scope of Quarkus extensions goes well beyond "just" importing dependent libraries. They can significantly increase the application's performance, help developers be more productive while developing their applications, integrate complex dependencies much easier, and simplify the application's source code.&lt;/p&gt; &lt;p&gt;Examples of Quarkus extensions include the Java Database Connectivity (JDBC) libraries, OpenAPI generators, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; manifest generators, and Apache Camel components. But there are many, many more.  &lt;/p&gt; &lt;h2&gt;Quarkus extensions enhance app performance&lt;/h2&gt; &lt;p&gt;A significant advantage of using Quarkus extensions is that they integrate seamlessly into the Quarkus architecture to take advantage of its superb build time optimization phase. This way, extensions can prescribe how to load and scan your application's bytecode (including the dependencies) and configuration optimally during the build augmentation stage instead of during startup time. This allows for significantly reduced resource usage and a much faster startup time for the application's dependent libraries and technologies, just like the core Quarkus components.&lt;/p&gt; &lt;p&gt;Extensions also get access to the preparation phase for GraalVM native compilation so that they can leverage the advantages of natively compiled Quarkus binaries. Keep in mind that it is the extension's responsibility to make sure it is compatible with native compilation, so make sure to read its documentation. For a deeper understanding of how this works, check out the &lt;a href="https://quarkus.io/guides/writing-extensions#technical-aspect"&gt;Quarkus documentation on how to write an extension&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Quarkus extensions increase developer productivity and joy&lt;/h2&gt; &lt;p&gt;Aside from aiding the performance of your application, Quarkus extensions can give a significant boost to developer productivity and help make developing applications more enjoyable as well by not having to fiddle around with configurations, extend the Quarkus CLI, and being able to leverage Quarkus's Dev Mode.&lt;/p&gt; &lt;h3&gt;Dev Mode&lt;/h3&gt; &lt;p&gt;Quarkus Dev Mode is a feature that starts up the application locally (or even remotely) and provides a set of capabilities to allow developers to iterate and test their code changes quickly. Dev Mode does targeted hot reloads on the fly when you change your code. This has the benefit of not needing to do manual restarts or rebuilding the application each time you change something. &lt;/p&gt; &lt;p&gt;This benefit applies to extensions as well. They will only be (re-)loaded if they have been changed, added, or removed. Any dependent configuration of the extension, such as a database migration script or similar, will also have the same behavior. For example, if you include an &lt;code&gt;import.sql&lt;/code&gt;, the import statement will run only during Dev Mode startup or when the file gets changed.&lt;/p&gt; &lt;h3&gt;Dev Services&lt;/h3&gt; &lt;p&gt;Extensions can also leverage the &lt;a href="https://quarkus.io/guides/dev-services"&gt;Quarkus Dev Services&lt;/a&gt; capability. Dev Services provides an automated way to spin up dependent services, typically in a local &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; and usually using the &lt;a href="https://www.testcontainers.org/"&gt;Testcontainers&lt;/a&gt; project. Think of databases, Apache Kafka clusters, Keycloak, etc. Dev Services are wired into your application without further configuration, making them a powerful way to increase developer productivity because you don't have to worry about setting up complicated dependent services on your local machine.&lt;/p&gt; &lt;p&gt;To use Dev Services, all you have to do is add the relevant extension—e.g. &lt;code&gt;jdbc-postgresql&lt;/code&gt; or &lt;code&gt;camel-quarkus-kafka&lt;/code&gt; (Figure 1)—and when you start up Quarkus's &lt;a href="https://quarkus.io/guides/dev-mode-differences#dev-mode-features"&gt;Dev Mode&lt;/a&gt; (using the command &lt;code&gt;quarkus dev&lt;/code&gt; or &lt;code&gt;mvn quarkus:dev&lt;/code&gt;), the extension will start up a Dev Service automatically. Because Dev Services usually rely on containers, you will need to have a container runtime on your machine such as Podman or Docker to enjoy this feature (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-10_15-39-19.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-10_15-39-19.png?itok=nk0JGQ38" width="600" height="84" alt="adding the camel-quarkus-kafka extensions automatically starts up a Kafka Dev Service" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Adding the camel-quarkus-kafka extension automatically starts a Kafka Dev Service during Dev Mode.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-14_08-55-32.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-14_08-55-32.png?itok=bPoYfBjO" width="600" height="310" alt="Kafka Dev Service running in Podman Desktop" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The running Kafka Dev Services container (using an image provided by Redpanda) shown in Podman Desktop&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Dev UI&lt;/h3&gt; &lt;p&gt;When Quarkus runs in Dev Mode, it also serves a web application called the Dev UI, which, among other things, shows the extensions currently used by your application. Extensions can expose additional information and capabilities in the Dev UI. At the minimum, they show details of the extension or a link to further documentation (which can be very handy!).&lt;/p&gt; &lt;p&gt;Extensions can add actionable capabilities to the Dev UI as well (Figure 3). For many extensions, you can change the configuration on the fly through the Dev UI, and some extensions have purpose-built capabilities such as building a container image, deploying to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, accessing and interacting with the Swagger UI, or even adding records to a Kafka topic running in a Dev Service. Of course, these capabilities depend on the extension and what has been implemented for it.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-11_15-58-18.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-11_15-58-18.png?itok=VluaeRxq" width="600" height="240" alt="Quarkus Dev UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Sample extensions shown in the Dev UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Extensions with a related Dev Service will also display the configuration that was automatically generated by Quarkus when the Dev Service started up. This can be very useful for developers to determine what configuration settings are needed for a particular dependency, such as the data source's type, JDBC URL, username, and password, as you can see in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-14_11-41-05.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-14_11-41-05.png?itok=NctIAfQ6" width="600" height="426" alt="Quarkus Dev UI showing Kafka and Postgresql Dev Services" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Dev Services shown in the Quarkus Dev UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Quarkus CLI&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;quarkus&lt;/code&gt; command lets you create projects, manage extensions, and do essential build and development tasks using the underlying project build tool (e.g., Maven or Gradle). Extensions can extend the functionality of the Quarkus CLI commands by virtue of Quarkus CLI plug-ins (Figure 5). This plug-in system can dynamically add commands and subcommands to the CLI. Some plug-in extensions are available out of the box, which you can retrieve with the Quarkus &lt;code&gt;plug-in list-installable&lt;/code&gt; command. In addition, you can also install executable jars from community extensions using their Maven coordinates. &lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-21_14-08-29.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-21_14-08-29.png?itok=S-kbeDfo" width="600" height="104" alt="List of Quarkus CLI plugins" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: An example list of available Quarkus CLI plug-ins.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Quarkus extensions catalog and versioning&lt;/h2&gt; &lt;p&gt;Quarkus has an extensive catalog of official Quarkus extensions. To get an idea of the extensions that are available for Quarkus, take a look at the &lt;a href="https://quarkus.io/extensions/"&gt;quarkus.io extensions catalog page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There is no need to keep track of the versioning of any individual extension, thanks to the concept of the Quarkus platform. This concept promises that any combination of the Quarkus extensions within the platform can be used in the same application without causing conflict. This also means that the versions of the extensions are determined by the platform. In practice, this works through the use of a Quarkus platform BOM artifact, which is imported as a dependency on your project.&lt;/p&gt; &lt;p&gt;Thanks to this Quarkus platform concept, you don't have to explicitly define the version of each Quarkus extension (or the libraries they are acting as a wrapper for) in your &lt;code&gt;pom.xml&lt;/code&gt; or &lt;code&gt;build.gradle&lt;/code&gt; file, and when you upgrade your Quarkus version, your extension versions are also automatically updated. Bear in mind that the extensions catalog might also contain extensions that are not necessarily production-ready, so it's always a good practice to read up on the extension's documentation before using it.&lt;/p&gt; &lt;h2&gt;The Quarkiverse&lt;/h2&gt; &lt;p&gt;The &lt;a href="http://github.com/quarkiverse"&gt;Quarkiverse GitHub organization&lt;/a&gt; provides hosting for Quarkus extension projects—not only for the ones that are featured in the main Quarkus extensions catalog, but also for additional extensions provided by Quarkus community developers. These community extensions are fully maintained by the extension's independent development teams and are typically kept up to date so that they are compatible with the different Quarkus versions. However, there are no explicit guarantees, so ensuring the project is active and up-to-date before using them is essential.&lt;/p&gt; &lt;h2&gt;How to add extensions to your Quarkus project&lt;/h2&gt; &lt;p&gt;You can add a Quarkus extension in a few ways:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;If you're generating a new project using &lt;a href="https://code.quarkus.io/"&gt;code.quarkus.io&lt;/a&gt; or the supported Red Hat build of Quarkus, &lt;a href="https://code.quarkus.redhat.com/"&gt;code.quarkus.redhat.com&lt;/a&gt;, you can select the extensions from the list and they will automatically be added to the generated code starter.&lt;/li&gt; &lt;li aria-level="1"&gt;Using the &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus CLI&lt;/a&gt;, you can add extensions using the quarkus extension add command. For example, to add the OpenShift client extension to your project, you would type &lt;code&gt;quarkus extension add quarkus-openshift-client&lt;/code&gt;. (Quarkus will automatically resolve it to &lt;code&gt;quarkus.io:quarkus-openshift-client&lt;/code&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;If you're developing with VS Code or IntelliJ IDEA, you can also use the Quarkus plug-in to add extensions to your project, as illustrated in Figure 6.&lt;/li&gt; &lt;/ul&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-10_14-40-03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-10_14-40-03.png?itok=4FhkUJVT" width="600" height="201" alt="How to add extensions with Quarkus Plugin for VS Code" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: Select "Quarkus: Add extensions to current project" in the VS Code command palette.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Go forth and extend your Quarkus application!&lt;/h2&gt; &lt;p&gt;This was a brief introduction to Quarkus extensions. If you want to learn more about Quarkus, check out the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus overview on Red Hat Developer&lt;/a&gt;, or get started with Quarkus tutorials on &lt;a href="https://developers.redhat.com/learn"&gt;developers.redhat.com/learn&lt;/a&gt;. If you want to learn even more about how Quarkus extensions work, or perhaps how to build your own extension, you can find more information on &lt;a href="https://quarkus.io/guides/writing-extensions"&gt;quarkus.io&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/06/introduction-quarkus-extensions-java-dependencies" title="Quarkus extensions give Java dependencies superpowers"&gt;Quarkus extensions give Java dependencies superpowers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Dubois</dc:creator><dc:date>2023-09-06T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.3.2 released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-3-2-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-3-2-released/</id><updated>2023-09-06T00:00:00Z</updated><published>2023-09-06T00:00:00Z</published><summary type="html">Today, we released Quarkus 3.3.2, our second maintenance release for our 3.3 release train. It includes a bunch of bugfixes, together with documentation improvements. The startup performance/memory regression introduced in 3.3 mentioned in the 3.3.1 announcement should be fixed in this version. Update To update to Quarkus 3.3.2, we recommend...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-06T00:00:00Z</dc:date></entry><entry><title>A beginner's guide to Python containers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/05/beginners-guide-python-containers" /><author><name>Aine Keenan</name></author><id>ef06738c-3521-4668-8779-1dab80cd34e9</id><updated>2023-09-05T07:00:00Z</updated><published>2023-09-05T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; has emerged as a go-to language for students, new programmers, and experienced developers. This general-purpose programming language is dynamically typed, memory-managed, and supports multiple programming paradigms. Python is popular for web development, &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence and machine learning (AI/ML)&lt;/a&gt;, scripting for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;, and more.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers"&gt;Containers&lt;/a&gt; are a game-changing solution for packaging your code and dependencies, allowing for your application to run quickly and consistently across any environment. Using containers to support your Python application enables efficient development and deployment.&lt;/p&gt; &lt;p&gt;This article shows how to use containers to support your Python applications. We'll take a pre-configured application and build a Containerfile for it from scratch. Additionally, we will walk through the importance and implementation of each part.&lt;/p&gt; &lt;h2&gt;Example Flask application&lt;/h2&gt; &lt;p&gt;This tutorial uses an &lt;a href="https://github.com/ainekeenan/recipe-image.git"&gt;example Python Flask application&lt;/a&gt;. When you input a URL for a recipe, it will return links to other recipes with similar "rare" ingredients.&lt;/p&gt; &lt;p&gt;If you would like to follow along, &lt;a href="https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository"&gt;clone the example repository&lt;/a&gt; and &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;/files&lt;/code&gt; directory.&lt;/p&gt; &lt;h2&gt;Install Podman&lt;/h2&gt; &lt;p&gt;Podman is a cloud-native, daemonless tool for developing, managing, and running Linux containers. Podman manages the entire container ecosystem from pulling, building, running, and pushing an image. Podman's features are based on secure practices, but also minimize the friction between your local development environment and production.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;[ Learn more: &lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt; ] &lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Our Python container will rely on Podman to build our image, run the image, and manage the running container.&lt;/p&gt; &lt;p&gt;To install Podman:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Head to the &lt;a href="https://podman-desktop.io/"&gt;Podman Desktop&lt;/a&gt; site and select the download for your environment.&lt;/li&gt; &lt;li aria-level="1"&gt;Launch Podman Desktop.&lt;/li&gt; &lt;li aria-level="1"&gt;If not already installed, Podman Desktop will prompt you to download &lt;a href="https://docs.podman.io/en/latest/"&gt;Podman&lt;/a&gt;, the underlying container engine.&lt;/li&gt; &lt;li aria-level="1"&gt;Start the Podman machine when prompted by Podman Desktop.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now that we've configured our container engine, let’s build a Containerfile.&lt;/p&gt; &lt;h2&gt;Container parts&lt;/h2&gt; &lt;p&gt;A container will include everything your application needs to run on any machine. The main elements (illustrated in Figure 1) include a base image or platform operating system that you want the system calls to virtualize. You also need your application code. Finally, you need all dependencies, like specific versions of programming language runtimes and libraries your code needs to run.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/container_layers_.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/container_layers_.png?itok=L0W6WMOn" width="600" height="318" alt="Layers of a container: Base image, dependencies, and application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Anatomy of a container: Application code, dependencies, and a base image.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The repository for our recipe website application holds this Containerfile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;FROM python LABEL maintainer="akeenan@redhat.com" COPY dependencies.txt dependencies.txt RUN pip3 install -r dependencies.txt RUN python3 -m spacy download en_core_web_sm COPY . . CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The file &lt;code&gt;dependencies.txt&lt;/code&gt; contains the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;flask beautifulsoup4 requests numpy pandas clean-text spacy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, let's break down the Containerfile and explain why each part is necessary.&lt;/p&gt; &lt;h3&gt;FROM&lt;/h3&gt; &lt;p&gt;&lt;code&gt;FROM python&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The first necessary instruction for any Containerfile is &lt;code&gt;FROM&lt;/code&gt;. &lt;code&gt;FROM&lt;/code&gt; tells the container system to start a new build stage, or a new creation of an image, using the specified image as a base image.&lt;/p&gt; &lt;p&gt;&lt;code&gt;#Syntax for FROM &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM [--platform=] [AS ] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM [--platform=] [:] [AS ] &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;FROM [--platform=] [@] [AS ]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can use the &lt;code&gt;:&lt;tag&gt;&lt;/code&gt; to specify a specific version of an image. For example, to receive the Python version 3.9.17 image, the image argument would look like this: &lt;code&gt;python:3.9.17&lt;/code&gt;. Tags can be reused, so there is no guarantee the &lt;code&gt;python:latest&lt;/code&gt; image will be the same as &lt;code&gt;python:latest&lt;/code&gt; image in 2 months.&lt;/p&gt; &lt;p&gt;If you want to ensure that you are using the same image at all times, you can specify a digest. A digest is a specific image, specified by an ID, that will not change over time. You can obtain a digest ID in the following ways:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;By running &lt;code&gt;podman manifest inspect python:&lt;tag&gt;&lt;/code&gt; (Figure 2)&lt;/li&gt; &lt;li aria-level="1"&gt;By downloading it from various image repository hubs: &lt;ul&gt;&lt;li aria-level="2"&gt;&lt;a href="https://quay.io/repository/centos7/python-38-centos7?tab=tags"&gt;Quay&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;a href="https://hub.docker.com/_/python"&gt;Docker Hub&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;a href="https://catalog.redhat.com/software/containers/ubi8/python-38/5dde9cacbed8bd164a0af24a"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If a tag or digest is not specified, the &lt;code&gt;:latest&lt;/code&gt; tag will be used by default.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podman_digest_inspect.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podman_digest_inspect.png?itok=UdQ-rEjC" width="600" height="378" alt="Running Podman manifest inspect python:3.9.17 in the terminal will display the digest ID." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Running the podman manifest inspect command in the CLI to get the digest ID.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h4&gt;FROM scratch&lt;/h4&gt; &lt;p&gt;If you do not want to use a base image—perhaps you want to containerize your own underlying operating system (OS), or you have a very minimal image—you have the option to use &lt;code&gt;FROM scratch&lt;/code&gt;. The &lt;code&gt;scratch&lt;/code&gt; image does not include any files or folders; instead, it tells the container system to have the next container instruction to be the first filesystem layer of the image.&lt;/p&gt; &lt;p&gt;Each base image, except for &lt;code&gt;scratch&lt;/code&gt;, includes an OS that it virtualizes. The default &lt;code&gt;python:&lt;version&gt;&lt;/code&gt; image virtualizes Debian GNU/Linux, while &lt;code&gt;python:&lt;version&gt;-alpine&lt;/code&gt; virtualizes Alpine Linux, and &lt;code&gt;python:&lt;version&gt;-windowsservercore&lt;/code&gt; virtualizes Windows Core OS.&lt;/p&gt; &lt;p&gt;Each operating system has a &lt;strong&gt;user space&lt;/strong&gt; and a &lt;strong&gt;kernel space&lt;/strong&gt;. User spaces are processes executed by a user in the operating system. The kernel space manages resources like RAM and Disk.&lt;/p&gt; &lt;p&gt;A container runs in the user space and accesses these resources by system calls. A container abstracting a certain OS will abstract the user space, and perform system calls to the host OS. In your container, the OS virtualizes Linux distributions by having their file systems packaged into a container filesystem. Sharing an underlying virtualized OS allows for a standard environment among containers.&lt;/p&gt; &lt;p&gt;For a Python application, the Python base image works well, including all up-to-date packages you may need. &lt;code&gt;FROM python&lt;/code&gt; will instruct the container engine to inherit the Python base image. The Python base image will have an underlying OS, the language (Python source code, compiled), the Python runtime dependencies, and &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If you have a specific OS you would like to virtualize, you can inherit that base image and download Python (more on that later in this article).&lt;/p&gt; &lt;h3&gt;LABEL&lt;/h3&gt; &lt;pre&gt; &lt;code&gt;LABEL maintainer="akeenan@redhat.com" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A label is used for metadata about a container, using a key=value format. In this label, we use a maintainer key, with the value of. Using a maintainer reference provides a way for developers to reach out to the maintainer if they have questions or feedback about the container. It also gives you a way to take credit for your work.&lt;/p&gt; &lt;h3&gt;COPY&lt;/h3&gt; &lt;p&gt;&lt;code&gt;#Copy (local source) (destination in container) &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;COPY dependencies.txt dependencies.txt&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now we have our base image and a reference to a maintainer. Our container has an owner, and the basics of the Python language.&lt;/p&gt; &lt;p&gt;The next step is to use &lt;code&gt;pip&lt;/code&gt; (which is included in the Python base image) to install our dependencies. Any extra library or package a developer downloads is a dependency. In our example Python program, we imported several dependencies, including Flask, BeautifulSoup, and pandas.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;from flask import Flask, render_template, request from bs4 import BeautifulSoup import numpy as np import pandas as pd import spacy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Steps to use the &lt;code&gt;COPY&lt;/code&gt; instruction:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt; &lt;p&gt;Create a file to hold the names of all dependencies. In the sample application, it is named &lt;code&gt;dependencies.txt&lt;/code&gt; and looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;#dependencies.txt flask beautifulsoup4 requests numpy pandas clean-text spacy&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;ol start="2"&gt;&lt;li aria-level="1"&gt;The instruction &lt;code&gt;COPY&lt;/code&gt; will copy your dependency file from your local source into the container. If your file is under another directory, you can include the path or use the &lt;a href="https://www.geeksforgeeks.org/docker-workdir-instruction/"&gt;WORKDIR&lt;/a&gt; instruction.&lt;/li&gt; &lt;li aria-level="1"&gt;Inside our local directory, the dependency file is there, named &lt;code&gt;dependencies.txt&lt;/code&gt;. The instruction &lt;code&gt;COPY dependencies.txt dependencies.txt&lt;/code&gt; will create a new file named &lt;code&gt;dependencies.txt&lt;/code&gt; in your container and copy over the contents.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;RUN&lt;/h3&gt; &lt;p&gt;&lt;code&gt;RUN pip3 install -r dependencies.txt &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;RUN python3 -m spacy download en_core_web_sm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The &lt;code&gt;RUN&lt;/code&gt; instruction will execute any commands needed in a new layer on top of our image.&lt;/p&gt; &lt;p&gt;We are using Python 3, so the command in the terminal to use &lt;code&gt;pip&lt;/code&gt; to install &lt;code&gt;&lt;insert-dependency-here&gt;&lt;/code&gt; is &lt;code&gt;pip3 install -r &lt;insert-dependency-here&gt;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;In our &lt;code&gt;dependencies.txt&lt;/code&gt; file, we have all packages and libraries that need to be downloaded using &lt;code&gt;pip&lt;/code&gt;. To shorten the installation process, &lt;code&gt;RUN pip3 install -r dependencies.txt&lt;/code&gt; will use &lt;code&gt;pip&lt;/code&gt; to install everything listed in our &lt;code&gt;dependencies.txt&lt;/code&gt; file.&lt;br /&gt;&lt;br /&gt; Additionally, for our sample program, we need to download &lt;code&gt;en_core_web_sm&lt;/code&gt; using the Python module space. This extra dependency is not installed using &lt;code&gt;pip&lt;/code&gt;, but we can use an extra &lt;code&gt;RUN&lt;/code&gt; command to complete the task.&lt;/p&gt; &lt;h3&gt;COPY&lt;/h3&gt; &lt;p&gt;&lt;code&gt;COPY . .&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now we have all dependencies downloaded using the &lt;code&gt;RUN&lt;/code&gt; instruction, it is time for our application files!&lt;/p&gt; &lt;p&gt;Our Containerfile is hosted in the same directory as our application files. &lt;code&gt;COPY . .&lt;/code&gt; copies our local directory outside of the container into the local directory inside the container.&lt;/p&gt; &lt;h3&gt;CMD&lt;/h3&gt; &lt;p&gt;&lt;code&gt;CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Every Containerfile must have one—and only one—&lt;code&gt;CMD&lt;/code&gt; instruction for the container to start. &lt;code&gt;CMD&lt;/code&gt; provides a default command for an executing container.&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host=0.0.0.0&lt;/code&gt; makes a non-routable meta-address of 0.0.0.0 for the host. It means that the mapping is valid for all addresses/interfaces of the host.&lt;/p&gt; &lt;p&gt;Normally, you would use the command &lt;code&gt;python3 -m flask run&lt;/code&gt; to run a Flask application. The same command is used here, in &lt;code&gt;&lt;code&gt;CMD ["executable","param1","param2"]&lt;/code&gt;&lt;/code&gt; format.&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host=0.0.0.0&lt;/code&gt; makes a non-routable meta-address of 0.0.0.0 for the host. It means that the mapping is valid for all addresses/interfaces of the host.&lt;/p&gt; &lt;h2 id="alternate_images-h2"&gt;Alternate images&lt;/h2&gt; &lt;p&gt;While the Containerfile above includes the official Python image, users can create a Containerfile to support Python in many different ways. Below, the first Containerfile inherits the &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; Universal Base Image, and then installs Python. The second Containerfile inherits a base image that has combined RHEL and Python dependencies. Users can customize their Python Containerfile to work best for their dependencies and target environment. &lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;#Alternate images that work! #Red Hat Enterprise Linux UBI, then installing Python FROM redhat/ubi8 LABEL maintainer="akeenan@redhat.com" RUN yum -y install python39 COPY dependencies.txt dependencies.txt RUN pip3 install -r dependencies.txt RUN python3 -m spacy download en_core_web_sm COPY . . CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"] EXPOSE 5000 &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code&gt;#RHEL UBI + Python in base image FROM registry.access.redhat.com/ubi8/python-311:1-13 LABEL maintainer="akeenan@redhat.com" COPY dependencies.txt dependencies.txt RUN pip3 install -r dependencies.txt RUN python3 -m spacy download en_core_web_sm COPY . . CMD ["python3", "-m" , "flask", "run", "--host=0.0.0.0"] EXPOSE 5000&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Build and run the container&lt;/h2&gt; &lt;p&gt;Now that you've written your image, it's time to build and run it. We'll walk through a few different ways to do this.&lt;/p&gt; &lt;h3&gt;Command-line interface&lt;/h3&gt; &lt;p&gt;In the command-line terminal, in the Containerfile directory, run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# should have your repo cloned # ensure you are in the files directory (Cd files) podman build --tag python-podman . podman run --publish 5000:5000 python-podman&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;podman build --tag python-podman .&lt;/code&gt; tells Podman to build the image in the current directory and name the image &lt;code&gt;python-podman&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;podman run --publish 5000:5000 python-podman&lt;/code&gt; will run the &lt;code&gt;python-podman&lt;/code&gt; image, and publish it on port 5000 outside of the container.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;--publish&lt;/code&gt; flag has syntax &lt;code&gt;(outside container): (inside container)&lt;/code&gt;. All Flask applications run on default port 5000, so here we connect our local host (address 127.0.0.1) port 5000 to container port 5000 on address 0.0.0.0.&lt;/p&gt; &lt;p&gt;Now, head to &lt;a href="http://127.0.0.1:5000/"&gt;http://127.0.0.1:5000&lt;/a&gt;, as seen in Figure 5, and enjoy your application! &lt;code&gt; &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Podman Desktop&lt;/h3&gt; &lt;p&gt;Add to the Containerfile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;EXPOSE 5000&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When we use the command-line interface, we force our application to be exposed on the localhost port 5000 using the command &lt;code&gt;--publish 5000:5000&lt;/code&gt;. On Podman Desktop, we just use the GUI, so including this line on our Containerfile will force our application to be exposed on the localhost port 5000.&lt;br /&gt;&lt;br /&gt; There are multiple ways to run a container file on Podman Desktop:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Build from Containerfile (Figure 3).&lt;/li&gt; &lt;li aria-level="1"&gt;Build on local terminal.&lt;/li&gt; &lt;li aria-level="1"&gt;Build from Quay.&lt;/li&gt; &lt;/ul&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_from_contianer_file.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_from_contianer_file.png?itok=uhAUCH_q" width="600" height="395" alt="Using Podman Desktop to build a image from Containerfile" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Using Podman Desktop to build a image from a Containerfile.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h4&gt;Build from Containerfile&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Navigate in Podman Desktop to the&lt;strong&gt; Images&lt;/strong&gt; tab and select &lt;strong&gt;Build an Image&lt;/strong&gt;, as shown in Figure 4. &lt;ol&gt;&lt;li aria-level="2"&gt;Select the location of your Containerfile.&lt;/li&gt; &lt;li aria-level="2"&gt;Select the location of your build directory.&lt;/li&gt; &lt;li aria-level="2"&gt;Name the image.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Build&lt;/strong&gt;, let the image build, and then select &lt;strong&gt;Done&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click the play button to start the container. &lt;ol&gt;&lt;li aria-level="2"&gt;Name the container.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the application by selecting the &lt;strong&gt;…&lt;/strong&gt; icon and &lt;strong&gt;Open in Browser&lt;/strong&gt;. You will be redirected to localhost:5000, as seen in Figure 5.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/nvm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/nvm.png?itok=Rq8CkSBs" width="600" height="395" alt="The Images tab in the Podman Desktop UI with the Pull an image and Build an image buttons." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The Images tab in the Podman Desktop where you can pull and build images.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h4&gt;Build on local terminal&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;In your build context directory, run: &lt;code&gt;podman build --tag python-podman .&lt;/code&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;In Podman Desktop, under the &lt;strong&gt;Images&lt;/strong&gt; tab, you will have a image called &lt;code&gt;localhost/python-podman.&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click the play button to start the container. &lt;ol&gt;&lt;li aria-level="2"&gt;Name the container.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the application by selecting the &lt;strong&gt;…&lt;/strong&gt; icon and &lt;strong&gt;Open in Browser&lt;/strong&gt;. You will be redirected to localhost:5000, as seen in Figure 5.&lt;/li&gt; &lt;/ol&gt;&lt;h4&gt;Build from Quay (or another image repository)&lt;/h4&gt; &lt;p&gt;If you need to set up your Quay repository, refer to the instructions in &lt;a href="https://docs.quay.io/guides/create-repo.html#:~:text=via%20the%20UI-,To%20create%20a%20repository%20in%20the%20Quay.io%20UI%2C%20click,the%20'Create%20Repository'%20button."&gt;this guide&lt;/a&gt;. &lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;In Podman Desktop, under the &lt;strong&gt;Images&lt;/strong&gt; tab, select the &lt;strong&gt;Pull an Image&lt;/strong&gt; icon, as shown in Figure 4. &lt;ol&gt;&lt;li aria-level="2"&gt;Enter the name of your image. It will look like this: &lt;code&gt;quay.io/&lt;username&gt;/&lt;image-name&gt;/&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Pull the image.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click the play button to start the container. &lt;ol&gt;&lt;li aria-level="2"&gt;Name the container.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Start&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Visit the application by selecting the &lt;strong&gt;…&lt;/strong&gt; icon and &lt;strong&gt;Open in Browser&lt;/strong&gt;. You will be redirected to localhost:5000, as seen in Figure 5.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/recipe_app_norm.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/recipe_app_norm.png?itok=DVAOA6zz" width="600" height="359" alt="The landing page for our web application on localhost:5000" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Our Flask application running on localhost:5000.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations! We successfully created a containerized Python application and saw how to run it in multiple ways. If you are working with multiple teams or multiple environments, you can now guarantee that your application will work the same every time you run it.&lt;/p&gt; &lt;p&gt;Whether you are a student, attending a hackathon, doing a personal project, or working on a business, Python and containers are a great place to start coding. Here are a few other resources to check out:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/get-started-with-your-developer-sandbox"&gt;Deploy a sample app on the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/build-and-deploy-a-quarkus-application-to-openshift-in-minutes"&gt;Build and deploy a Quarkus application to OpenShift in minutes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/using-openshift-pipelines"&gt;Using OpenShift Pipelines for automated builds and deployment&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When it comes time to scale your application, check out &lt;a href="https://developers.redhat.com/developer-sandbox/activities/learn-kubernetes-using-red-hat-developer-sandbox-openshift"&gt;Kubernetes for container orchestration&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/05/beginners-guide-python-containers" title="A beginner's guide to Python containers"&gt;A beginner's guide to Python containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Aine Keenan</dc:creator><dc:date>2023-09-05T07:00:00Z</dc:date></entry><entry><title type="html">Tutorial: Using Debezium JDBC Connector</title><link rel="alternate" href="https://www.mastertheboss.com/jboss-frameworks/debezium/tutorial-using-debezium-jdbc-connector/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jboss-frameworks/debezium/tutorial-using-debezium-jdbc-connector/</id><updated>2023-09-04T16:48:34Z</updated><content type="html">Debezium is an open-source CDC (Change Data Capture) platform that allows you to capture and stream database changes in real-time. The Debezium JDBC Connector enables you to monitor changes in relational databases, like PostgreSQL, and stream those changes to various downstream systems. In this tutorial, we will guide you through the process of using the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to use dynamic inventories in Ansible Automation</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/04/how-use-dynamic-inventories-ansible-automation" /><author><name>Deepankar Jain, Himanshu Yadav</name></author><id>1f49616c-32b2-4328-83a4-638f9319260d</id><updated>2023-09-04T07:00:00Z</updated><published>2023-09-04T07:00:00Z</published><summary type="html">&lt;p&gt;Dynamic inventories in the Red Hat Ansible Automation Platform revolutionize the way to manage infrastructure in the cloud. Instead of relying on static inventories that require manual updates when virtual machines (VMs) are launched, terminated, or replaced, dynamic inventories automatically discover and run VMs across any cloud provider. This means that when you delete an old VM and launch a new one, Ansible Automation Platform seamlessly adapts and performs actions on the updated infrastructure without requiring manual intervention.&lt;/p&gt; &lt;p&gt;By harnessing dynamic inventories, the Ansible Automation Platform empowers cloud administrators and DevOps teams to effortlessly manage and orchestrate on-demand cloud resources. This article explores the power of dynamic inventories, focusing on their utilization with AWS as the cloud provider. While applicable to any cloud provider, we showcase seamless AWS infrastructure management using dynamic inventories. We will demonstrate how to create EC2 instances, fetching details with dynamic inventory and running scripts for system health insights. By following these examples, you'll gain hands-on experience in effectively managing and monitoring your cloud infrastructure using dynamic inventories within the Ansible Automation Platform.&lt;/p&gt; &lt;h2&gt;Demo setup&lt;/h2&gt; &lt;p&gt;We will walk you through this hands-on example, creating three EC2 instances in AWS. We will then explore how to leverage dynamic inventory in the Ansible Automation Platform to automatically fetch the details of these instances. Using a playbook, we will execute tasks on these instances and observe the desired changes.&lt;/p&gt; &lt;p&gt;We will also delete one of the EC2 instances and spin up a new one, showcasing the dynamic nature of inventories. By syncing the inventory in the Ansible Automation Platform, we will observe how it seamlessly adapts to the changes, enabling us to easily continue managing the updated infrastructure.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;You must have an active AWS account.&lt;/li&gt; &lt;li aria-level="1"&gt;Generate the &lt;a href="https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html"&gt;access key and client secret for AWS&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Navigate to the &lt;strong&gt;Credentials tab&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Under the &lt;strong&gt;Add&lt;/strong&gt; &lt;strong&gt;button&lt;/strong&gt;, select &lt;strong&gt;Amazon Web Services&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Add your access key and secret key and save the credentials (Figure 1).&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-28_102411.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-28_102411.png?itok=2cfaeDZk" width="600" height="177" alt="A screenshot of the Ansible create AWS credentials page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Ansible create credentials page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;How to use dynamic inventory to manage AWS infrastructure&lt;/h2&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create three EC2 instances in the AWS console as follows:&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Log in to your AWS console and navigate to the EC2 sections.&lt;/li&gt; &lt;li aria-level="1"&gt;Under the EC2 console, click on &lt;strong&gt;Launch Instances&lt;/strong&gt; and create three instances with different names for Ubuntu image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Make sure to use a single private key for all three EC2 instances as shown in Figure 2.&lt;/p&gt; &lt;ol start="2"&gt;&lt;li aria-level="1"&gt;Create the credentials to SSH into the AWS machine as follows:&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Navigate to the credentials tab.&lt;/li&gt; &lt;li aria-level="1"&gt;Under the &lt;strong&gt;Add&lt;/strong&gt; &lt;strong&gt;button&lt;/strong&gt;, select machine.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the credential.&lt;/li&gt; &lt;li aria-level="1"&gt;Under &lt;strong&gt;Username&lt;/strong&gt;, enter “ubuntu” and enter “root” for &lt;strong&gt;Privilege Escalation Username&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Add your AWS SSH private key and click &lt;strong&gt;save&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_1.png?itok=s3tvTHNs" width="600" height="258" alt="Figure 2: Create machine credentials" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Create machine credentials.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="3"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Set up the inventory as follows:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Select the inventory from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on add and select add inventory.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the inventory and save it.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the sources from inventories and click on add.&lt;/li&gt; &lt;li aria-level="1"&gt;Give a name to the source.&lt;/li&gt; &lt;li aria-level="1"&gt;Under source, select Amazon EC2.&lt;/li&gt; &lt;li aria-level="1"&gt;In the &lt;strong&gt;Credentials field&lt;/strong&gt;, select the AWS credentials you created earlier.&lt;/li&gt; &lt;li aria-level="1"&gt;Save the sources and then click on the &lt;strong&gt;Sync&lt;/strong&gt; button.&lt;/li&gt; &lt;li aria-level="1"&gt;Now, navigate to the &lt;strong&gt;Groups&lt;/strong&gt; section and notice that &lt;strong&gt;aws_ec2&lt;/strong&gt; group is created, and your EC2 IPs will be available in the hosts (Figure 3).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_4.png?itok=yGB79Y3S" width="600" height="158" alt="Figure 4: AWS hosts in Inventory" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: AWS hosts in Inventory.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="4"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Create and Configure the project as follows:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Log in to the Ansible Automation Platform portal in the browser.&lt;/li&gt; &lt;li aria-level="1"&gt;Navigate to the &lt;strong&gt;Projects&lt;/strong&gt; tab under &lt;strong&gt;Resources&lt;/strong&gt; in the left pane.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Add&lt;/strong&gt; to create a new project.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the project and choose &lt;strong&gt;Git&lt;/strong&gt; as the source control type with URL https://github.com/redhat-developer-demos/Ansible-use-cases in the &lt;strong&gt;Source Control URL field&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Save the changes and wait for the operation to complete successfully.&lt;/li&gt; &lt;/ul&gt;&lt;ol start="5"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Create and Configure the job templates:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Go to the &lt;strong&gt;Templates&lt;/strong&gt; tab under &lt;strong&gt;Resources&lt;/strong&gt; in the left pane and click on the &lt;strong&gt;Add&lt;/strong&gt; button and select &lt;strong&gt;Job template&lt;/strong&gt; from the options.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name for the job you want to create and select the Demo-Inventory or Default inventory in the inventory section.&lt;/li&gt; &lt;li aria-level="1"&gt;In the &lt;strong&gt;Project&lt;/strong&gt; section, click on the project name you previously created and select the &lt;strong&gt;Dynamic Inventory in Ansible Automation Platform/get_sys_data.yml&lt;/strong&gt; file.&lt;/li&gt; &lt;li aria-level="1"&gt;In the credential section, select the machine category and choose the credentials for AWS.&lt;/li&gt; &lt;/ul&gt;&lt;ol start="6"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Launch the playbooks:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Launch the playbook to see the logs of the output as shown in Figure 4:&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_3_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_3_0.png?itok=7EuaOrQ7" width="600" height="251" alt="Figure 4: Playbook output" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The playbook output.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="7"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Delete an EC2 instance and spin up a new instance:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Now, let us delete the third machine and create a new ec2 instance.&lt;/li&gt; &lt;li aria-level="1"&gt;Under AWS console, navigate to the EC2 instance, delete any machine, and create a new instance with the previous configuration setup (Figure 5).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_5.png?itok=oDziDcxR" width="600" height="290" alt="Figure 5: Deleting an AWS machine" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Deleting an AWS machine.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="8"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Sync the inventory:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Navigate to the inventory section and click on the inventory you previously created.&lt;/li&gt; &lt;li aria-level="1"&gt;Under the &lt;strong&gt;Sources Tab&lt;/strong&gt;, select the name created earlier and click on sync.&lt;/li&gt; &lt;li aria-level="1"&gt;Further, we can also schedule the task to update the inventory after every interval.&lt;/li&gt; &lt;li aria-level="1"&gt;Notice that our hosts have been updated (Figure 6). It takes time from the AWS console to completely terminate your old EC2 instances, and it might show up under hosts.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_6.png?itok=lKq5HAJg" width="600" height="176" alt="Figure 6: The updated hosts under Ansible Automation Platform" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The updated hosts under Ansible Automation Platform.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="9"&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Launch the playbook with the updated inventory:&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Navigate to the &lt;strong&gt;Templates&lt;/strong&gt; section and select the job template created earlier.&lt;/li&gt; &lt;li aria-level="1"&gt;Launch the playbook (Figure 7).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dynamic_inventory_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/dynamic_inventory_7.png?itok=mx4FpLZ3" width="600" height="260" alt="Figure 7: The playbook output with updated inventory" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: The playbook output with updated inventory.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;Dynamic inventory not only simplifies inventory management but also enhances the agility and scalability of your workflows. In this article, we have demonstrated the power of efficient infrastructure automation by showcasing the creation and management of three EC2 instances in AWS and the utilization of dynamic inventory to seamlessly adapt to changes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started&lt;/a&gt; with Ansible Automation Platform by exploring interactive hands-on labs. &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Download Ansible Automation Platform&lt;/a&gt; at no cost and begin your automation journey. You can refer to &lt;a href="https://developers.redhat.com/e-books/choosing-automation-tool"&gt;An IT executive's guide to automation&lt;/a&gt; e-book for a better understanding of automation. Additionally, check out our series where we explain &lt;a href="https://developers.redhat.com/articles/2023/06/05/how-create-ec2-instance-aws-using-ansible-cli"&gt;how to create an EC2 instance in AWS using Ansible&lt;/a&gt;, empowering you to efficiently manage your cloud resources.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/04/how-use-dynamic-inventories-ansible-automation" title="How to use dynamic inventories in Ansible Automation"&gt;How to use dynamic inventories in Ansible Automation&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain, Himanshu Yadav</dc:creator><dc:date>2023-09-04T07:00:00Z</dc:date></entry><entry><title>Automate message queue deployment on JBoss EAP</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/01/automate-message-queue-deployment-jboss-eap" /><author><name>Romain Pelisse</name></author><id>94e338a7-dd9b-4b3a-b292-0941474d3d31</id><updated>2023-09-01T07:00:00Z</updated><published>2023-09-01T07:00:00Z</published><summary type="html">&lt;p&gt;For decades now, software projects have relied on messaging &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;APIs&lt;/a&gt; to exchange data. In the &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt;/Java EE ecosystem, this method of asynchronous communication has been standardized by the JMS specification. In many cases, individuals and organizations leverage &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) to act as message-oriented middleware (MOM), which facilitates the management of message queues and topics.&lt;/p&gt; &lt;p&gt;Messaging ensures that no messages are lost as they are transmitted from the client and delivered to interested parties. On top of that, JBoss EAP provides authentication and other security-focused capabilities on top of the management functions.&lt;/p&gt; &lt;p&gt;In this article, we'll show how to fully automate the setup of JBoss EAP and a JMS queue using &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; so that we can easily make this service available.&lt;/p&gt; &lt;h2&gt;1. Prerequisites and installation&lt;/h2&gt; &lt;h3&gt;1.1 Install Ansible&lt;/h3&gt; &lt;p&gt;First, we’ll set up our Ansible &lt;strong&gt;control&lt;/strong&gt; machine, which is where the automation will be executed. On this system, we need to install Ansible as the first step:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ sudo dnf install -y ansible-core&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the package name has changed recently from &lt;code&gt;ansible&lt;/code&gt; to &lt;code&gt;ansible-core&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;1.2 Configure Ansible to use Red Hat Automation Hub&lt;/h3&gt; &lt;p&gt;An extension to Ansible, an &lt;a href="https://docs.ansible.com/ansible/latest/collections_guide/index.html"&gt;Ansible collection&lt;/a&gt;, dedicated to Red Hat JBoss EAP is available from Automation Hub. Red Hat customers need to add credentials and the location for &lt;a href="https://www.ansible.com/products/automation-hub"&gt;Red Hat Automation Hub&lt;/a&gt; to their Ansible configuration file (&lt;code&gt;ansible.cfg&lt;/code&gt;) to be able to install the content using the &lt;a href="https://docs.ansible.com/ansible/latest/cli/ansible-galaxy.html"&gt;ansible-galaxy&lt;/a&gt; command-line tool.&lt;/p&gt; &lt;p&gt;Be sure to replace the with the API token you retrieved from Automation Hub. For more information about using &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;Red Hat Automation Hub, please refer to the associated documentation&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;#ansible.cfg: [defaults] host_key_checking = False retry_files_enabled = False nocows = 1 [inventory] # fail more helpfully when the inventory file does not parse (Ansible 2.4+) unparsed_is_failed=true [galaxy] server_list = automation_hub, galaxy [galaxy_server.galaxy] url=https://galaxy.ansible.com/ [galaxy_server.automation_hub] url=https://cloud.redhat.com/api/automation-hub/ auth_url=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token token=&lt;paste-your-token-here&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;1.3 Install the Ansible collection for JBoss EAP&lt;/h3&gt; &lt;p&gt;With this configuration, we can now install the Ansible collection for JBoss EAP (&lt;code&gt;redhat.eap&lt;/code&gt;) available on Red Hat Ansible Automation Hub:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install redhat.eap Starting galaxy collection install process Process install dependency map Starting collection install process Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/redhat-eap-1.3.4.tar.gz to /root/.ansible/tmp/ansible-local-2529rs7zh7/tmps_4n2eyj/redhat-eap-1.3.4-lr8dvcxo Installing 'redhat.eap:1.3.4' to '/root/.ansible/collections/ansible_collections/redhat/eap' Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/redhat-runtimes_common-1.1.0.tar.gz to /root/.ansible/tmp/ansible-local-2529rs7zh7/tmps_4n2eyj/redhat-runtimes_common-1.1.0-o6qfkgju redhat.eap:1.3.4 was installed successfully Installing 'redhat.runtimes_common:1.1.0' to '/root/.ansible/collections/ansible_collections/redhat/runtimes_common' Downloading https://console.redhat.com/api/automation-hub/v3/plugin/ansible/content/published/collections/artifacts/ansible-posix-1.5.4.tar.gz to /root/.ansible/tmp/ansible-local-2529rs7zh7/tmps_4n2eyj/ansible-posix-1.5.4-4pgukpuo redhat.runtimes_common:1.1.0 was installed successfully Installing 'ansible.posix:1.5.4' to '/root/.ansible/collections/ansible_collections/ansible/posix' ansible.posix:1.5.4 was installed successfully &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As we will describe a little later on, this extension for Ansible will manage the entire installation and configuration of the Java application server on the target systems.&lt;/p&gt; &lt;h3&gt;1.4 Inventory file&lt;/h3&gt; &lt;p&gt;Before we can start using our collection, we need to provide the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;inventory of targets to Ansible&lt;/a&gt;. There are several ways to provide this information to the automation tool, but for the purposes of this article, we elected to use a simple ini-formatted inventory file.&lt;/p&gt; &lt;p&gt;To easily reproduce this article's demonstration, you can use the same control node as the target. This also removes the need to deploy the required SSH key on all the systems involved. To do so, simply use the following inventory file by creating a file called &lt;code&gt;inventory&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;[all] localhost ansible_connection=local [messaging_servers] localhost ansible_connection=local&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;2. Deploying JBoss EAP&lt;/h2&gt; &lt;h3&gt;2.1 JBoss EAP installation&lt;/h3&gt; &lt;p&gt;Before we configure the JMS queues that will be configured by Ansible, we'll first deploy JBoss EAP. Once the server is successfully running on the target system, we'll adjust the automation to add the required configuration to set up the messaging layer. This is purely for didactic purposes.&lt;/p&gt; &lt;p&gt;Since we can leverage the content of the &lt;code&gt;redhat.eap&lt;/code&gt; collection, the playbook to install EAP and set it up as systemd service on the target system is minimal. Create a file called &lt;code&gt;eap_jms.yml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a JBoss EAP" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' collections: - redhat.eap roles: - eap_install - eap_systemd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the Ansible collection for JBoss EAP will also take care of downloading the required assets from the Red Hat Customer Portal (the archive containing the Java app server files). However, one does need to provide the credentials associated with a service account. A Red Hat customer can manage service accounts using the hybrid cloud console. Within this portal, on the &lt;a href="https://console.redhat.com/application-services/service-accounts"&gt;service accounts tab&lt;/a&gt;, you can create a new service account if one does not already exist.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The values obtained from the hybrid cloud console are sensitive and should be managed accordingly. For the purpose of this article, the value is passed to the &lt;code&gt;ansible-playbook&lt;/code&gt; command line. Alternatively, &lt;a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html"&gt;ansible-vault&lt;/a&gt; could be used to enforce additional defense mechanisms:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory -e rhn_username=&lt;client_id&gt; -e rhn_password=&lt;client_secret&gt; eap_jms.yml PLAY [Deploy a JBoss EAP] ****************************************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [redhat.eap.eap_install : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_install : Ensure prerequirements are fullfilled.] ********* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/prereqs.yml for localhost TASK [redhat.eap.eap_install : Validate credentials] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Check that required packages list has been provided.] *** ok: [localhost] TASK [redhat.eap.eap_install : Prepare packages list] ************************** skipping: [localhost] TASK [redhat.eap.eap_install : Add JDK package java-11-openjdk-headless to packages list] *** ok: [localhost] TASK [redhat.eap.eap_install : Install required packages (4)] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required local user exists.] ************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/user.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_install : Ensure workdir /opt/jboss_eap/ exists.] ********* ok: [localhost] TASK [redhat.eap.eap_install : Ensure archive_dir /opt/jboss_eap/ exists.] ***** ok: [localhost] TASK [redhat.eap.eap_install : Ensure server is installed] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Check local download archive path] ************** ok: [localhost] TASK [redhat.eap.eap_install : Set download paths] ***************************** ok: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from website: https://github.com/eap/eap/releases/download] *** skipping: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from RHN] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install/rhn.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [Download JBoss EAP from CSP] ********************************************* TASK [redhat.eap.eap_utils : Check arguments] ********************************** ok: [localhost] TASK [redhat.eap.eap_utils : Retrieve product download using JBoss Network API] *** ok: [localhost] TASK [redhat.eap.eap_utils : Determine install zipfile from search results] **** ok: [localhost] TASK [redhat.eap.eap_utils : Download Red Hat Single Sign-On] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Install server using RPM] *********************** skipping: [localhost] TASK [redhat.eap.eap_install : Check downloaded archive] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Copy archive to target nodes] ******************* changed: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Verify target archive state: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Read target directory information: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Extract files from /opt/jboss_eap//jboss-eap-7.4.0.zip into /opt/jboss_eap/.] *** changed: [localhost] TASK [redhat.eap.eap_install : Note: decompression was not executed] *********** skipping: [localhost] TASK [redhat.eap.eap_install : Read information on server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Check state of server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Set instance name] ****************************** ok: [localhost] TASK [redhat.eap.eap_install : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_install : Deploy configuration] *************************** changed: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for cumulative patch application are provided.] *** skipping: [localhost] TASK [Apply latest cumulative patch] ******************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for elytron adapter are provided.] *** skipping: [localhost] TASK [Install elytron adapter] ************************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Install server using Prospero] ****************** skipping: [localhost] TASK [redhat.eap.eap_install : Check eap install directory state] ************** ok: [localhost] TASK [redhat.eap.eap_install : Validate conditions] **************************** ok: [localhost] TASK [Ensure firewalld configuration allows server port (if enabled).] ********* skipping: [localhost] TASK [redhat.eap.eap_systemd : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check current EAP patch installed] ************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments for yaml configuration] ********* skipping: [localhost] TASK [Ensure required local user and group exists.] **************************** TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_systemd : Set destination directory for configuration] **** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance destination directory for configuration] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set base directory for instance] **************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set bind address] ******************************* ok: [localhost] TASK [redhat.eap.eap_systemd : Create basedir /opt/jboss_eap/jboss-eap-7.4//standalone for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Create deployment directories for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Include YAML configuration extension] *********** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check YAML configuration is disabled] *********** ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd envfile destination] **************** ok: [localhost] TASK [redhat.eap.eap_systemd : Determine JAVA_HOME for selected JVM RPM] ******* ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd unit file destination] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy service instance configuration: /etc//eap.conf] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy Systemd configuration for service: /usr/lib/systemd/system/eap.service] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Ensure service is started] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance eap state to started] ************** changed: [localhost] PLAY RECAP ********************************************************************* localhost : ok=59 changed=6 unreachable=0 failed=0 skipped=22 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the playbook has been successfully executed, we can confirm that the application server is running on the target system using the systemctl command :&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# systemctl status eap ● eap.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/eap.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2023-05-23 14:01:51 UTC; 1min 25s ago Main PID: 1563 (standalone.sh) Tasks: 84 (limit: 1638) Memory: 376.9M CGroup: /system.slice/eap.service ├─1563 /bin/sh /opt/jboss_eap/jboss-eap-7.4/bin/standalone.sh -c eap.xml -b 0.0.0.0 -bmanagement 127.0.0.1 -Djboss.bind.address.private=127.0.0.1 -Djboss.default.multicast.address=230.0.0.4 -Djboss.server.config.dir=/opt/jboss_eap/jboss-eap-7.4//standalone/configuration/ -Djboss.server.base.dir=/opt/jboss_eap/jboss-eap-7.4//standalone -Djboss.tx.node.id=eap -Djboss.&gt; └─1706 /usr/lib/jvm/java-11-openjdk-11.0.19.0.7-4.el8.x86_64/bin/java -D[Standalone] -server -Xlog:gc*:file=/opt/jboss_eap/jboss-eap-7.4/standalone/log/gc.log:time,uptimemillis:filecount=5,filesize=3M -Xmx1024M -Xms512M --add-exports=java.base/sun.nio.ch=ALL-UNNAMED --add-exports=jdk.unsupported/sun.misc=ALL-UNNAMED --add-exports=jdk.unsupported/sun.reflect=ALL-UNNA&gt; May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,778 INFO [org.jboss.as.connector.subsystems.datasources] (MSC service thread 1-3) WFLYJCA0001: Bound data source [java:jboss/datasources/ExampleDS] May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,852 INFO [org.jboss.as.patching] (MSC service thread 1-3) WFLYPAT0050: JBoss EAP cumulative patch ID is: base, one-off patches include: none May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,862 WARN [org.jboss.as.domain.management.security] (MSC service thread 1-4) WFLYDM0111: Keystore /opt/jboss_eap/jboss-eap-7.4/standalone/configuration/application.keystore not found, it will be auto generated on first use with a self signed certificate for host localhost May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,867 INFO [org.jboss.as.server.deployment.scanner] (MSC service thread 1-5) WFLYDS0013: Started FileSystemDeploymentService for directory /opt/jboss_eap/jboss-eap-7.4/standalone/deployments May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,882 INFO [org.wildfly.extension.undertow] (MSC service thread 1-7) WFLYUT0006: Undertow HTTPS listener https listening on [0:0:0:0:0:0:0:0]:8443 May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,920 INFO [org.jboss.ws.common.management] (MSC service thread 1-4) JBWS022052: Starting JBossWS 5.4.2.Final-redhat-00001 (Apache CXF 3.3.9.redhat-00001) May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,989 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming server May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,991 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: JBoss EAP 7.4.0.GA (WildFly Core 15.0.2.Final-redhat-00001) started in 2282ms - Started 317 of 556 services (343 services are lazy, passive or on-demand) May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,992 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/management May 23 14:01:53 9b0f94a0c312 standalone.sh[1706]: 14:01:53,992 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;2.2 Validating the installation&lt;/h3&gt; &lt;p&gt;Before going any further with our automation, we will be thorough and add a validation step to double-check that the application server is not only running but also functional. This will ensure, down the road, that any JMS-related issue only affects this subsystem.&lt;/p&gt; &lt;p&gt;The Ansible collection for JBoss EAP comes with a handy role, called &lt;code&gt;eap_validation&lt;/code&gt;, for this purpose, so it's fairly easy to add this step to our playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a JBoss EAP" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' collections: - redhat.eap roles: - eap_install - eap_systemd - eap_validation&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's execute our playbook once again and observe the execution of this validation step:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory -e rhn_username=&lt;client_id&gt; -e rhn_password=&lt;client_secret&gt; eap_jms.yml PLAY [Deploy a JBoss EAP] ****************************************************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [redhat.eap.eap_install : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_install : Ensure prerequirements are fullfilled.] ********* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/prereqs.yml for localhost TASK [redhat.eap.eap_install : Validate credentials] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Check that required packages list has been provided.] *** ok: [localhost] TASK [redhat.eap.eap_install : Prepare packages list] ************************** skipping: [localhost] TASK [redhat.eap.eap_install : Add JDK package java-11-openjdk-headless to packages list] *** ok: [localhost] TASK [redhat.eap.eap_install : Install required packages (4)] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required local user exists.] ************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/user.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** changed: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ changed: [localhost] TASK [redhat.eap.eap_install : Ensure workdir /opt/jboss_eap/ exists.] ********* changed: [localhost] TASK [redhat.eap.eap_install : Ensure archive_dir /opt/jboss_eap/ exists.] ***** ok: [localhost] TASK [redhat.eap.eap_install : Ensure server is installed] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Check local download archive path] ************** ok: [localhost] TASK [redhat.eap.eap_install : Set download paths] ***************************** ok: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from website: https://github.com/eap/eap/releases/download] *** skipping: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from RHN] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install/rhn.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [Download JBoss EAP from CSP] ********************************************* TASK [redhat.eap.eap_utils : Check arguments] ********************************** ok: [localhost] TASK [redhat.eap.eap_utils : Retrieve product download using JBoss Network API] *** ok: [localhost] TASK [redhat.eap.eap_utils : Determine install zipfile from search results] **** ok: [localhost] TASK [redhat.eap.eap_utils : Download Red Hat Single Sign-On] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Install server using RPM] *********************** skipping: [localhost] TASK [redhat.eap.eap_install : Check downloaded archive] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Copy archive to target nodes] ******************* changed: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Verify target archive state: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Read target directory information: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Extract files from /opt/jboss_eap//jboss-eap-7.4.0.zip into /opt/jboss_eap/.] *** changed: [localhost] TASK [redhat.eap.eap_install : Note: decompression was not executed] *********** skipping: [localhost] TASK [redhat.eap.eap_install : Read information on server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Check state of server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Set instance name] ****************************** ok: [localhost] TASK [redhat.eap.eap_install : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_install : Deploy configuration] *************************** changed: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for cumulative patch application are provided.] *** skipping: [localhost] TASK [Apply latest cumulative patch] ******************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for elytron adapter are provided.] *** skipping: [localhost] TASK [Install elytron adapter] ************************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Install server using Prospero] ****************** skipping: [localhost] TASK [redhat.eap.eap_install : Check eap install directory state] ************** ok: [localhost] TASK [redhat.eap.eap_install : Validate conditions] **************************** ok: [localhost] TASK [Ensure firewalld configuration allows server port (if enabled).] ********* skipping: [localhost] TASK [redhat.eap.eap_systemd : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check current EAP patch installed] ************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments for yaml configuration] ********* skipping: [localhost] TASK [Ensure required local user and group exists.] **************************** TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_systemd : Set destination directory for configuration] **** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance destination directory for configuration] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set base directory for instance] **************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set bind address] ******************************* ok: [localhost] TASK [redhat.eap.eap_systemd : Create basedir /opt/jboss_eap/jboss-eap-7.4//standalone for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Create deployment directories for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Include YAML configuration extension] *********** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check YAML configuration is disabled] *********** ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd envfile destination] **************** ok: [localhost] TASK [redhat.eap.eap_systemd : Determine JAVA_HOME for selected JVM RPM] ******* ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd unit file destination] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy service instance configuration: /etc//eap.conf] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy Systemd configuration for service: /usr/lib/systemd/system/eap.service] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Ensure service is started] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance eap state to started] ************** changed: [localhost] TASK [redhat.eap.eap_validation : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of user: eap] ***************** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of group: eap.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Wait for HTTP port 8080 to become available.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Check if web connector is accessible] ******** ok: [localhost] TASK [redhat.eap.eap_validation : Populate service facts] ********************** ok: [localhost] TASK [redhat.eap.eap_validation : Check if service is running] ***************** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [redhat.eap.eap_validation : Verify server's internal configuration] ****** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/core-service=server-environment:read-attribute(name=start-gracefully)'}) included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'}) TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /core-service=server-environment:read-attribute(name=start-gracefully)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/core-service=server-environment:read-attribute(name=start-gracefully)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure yaml setup] *************************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/yaml_setup.yml for localhost TASK [Check standard-sockets configuration settings] *************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=mail-smtp:read-attribute(name=host)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of standard-sockets configuration settings] *** ok: [localhost] TASK [Check ejb configuration settings] **************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ejb3:read-attribute(name=default-resource-adapter-name)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ejb configuration settings] *** ok: [localhost] TASK [Check ee configuration settings] ***************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ee/service=default-bindings:read-attribute(name=jms-connection-factory)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ee configuration settings] *** ok: [localhost] PLAY RECAP ********************************************************************* localhost : ok=98 changed=9 unreachable=0 failed=0 skipped=24 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the execution of the playbook completed without error, validation of the application server passed successfully.&lt;/p&gt; &lt;h2&gt;3 Deploying JMS queues on JBoss EAP using Ansible&lt;/h2&gt; &lt;h3&gt;3.1 Changing EAP configuration&lt;/h3&gt; &lt;p&gt;Because the JMS subsystem is not used in the default JBoss EAP server configuration (&lt;code&gt;standalone.xml&lt;/code&gt;), we also need to use a different profile (&lt;code&gt;standalone-alone.xml&lt;/code&gt;). This is why, in the playbook above, we are specifying the required configuration profile:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a JBoss EAP" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' collections: - redhat.eap roles: - eap_install - eap_systemd - eap_validation&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;3.2 Leveraging the YAML config feature of EAP using Ansible&lt;/h3&gt; &lt;p&gt;In the previous section, JBoss EAP was installed and configured as a systemd service on the target systems. Now, we will update this automation to change the configuration of the app server to ensure a JMS queue is deployed and made available.&lt;/p&gt; &lt;p&gt;In order to accomplish this goal, we just need to provide a YAML definition with the appropriate configuration for the JMS subsystem of JBoss EAP. This configuration file is used by the app server, on boot, to update its configuration.&lt;/p&gt; &lt;p&gt;To achieve this, we need to add another file to our project that we named &lt;code&gt;jms_configuration.yml.j2&lt;/code&gt;. While the content of the file itself is YAML, the extension is &lt;code&gt;.j2&lt;/code&gt; because it's a jinja2 template, which allows us to take advantage of the advanced, dynamic capabilities provided by Ansible.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;jms_configuration.yml.j2: wildfly-configuration: subsystem: messaging-activemq: server: default: jms-queue: {{ queue.name }}: entries: - '{{ queue.entry }}' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Below, you'll see the playbook updated with all the required parameters to deploy the JMQ queue on JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Deploy a Red Hat JBoss EAP server and set up a JMS Queue" hosts: messaging_servers vars: eap_apply_cp: true eap_version: 7.4.0 eap_offline_install: false eap_config_base: 'standalone-full.xml' eap_enable_yml_config: True queue: name: MyQueue entry: 'java:/jms/queue/MyQueue' eap_yml_configs: - jms_configuration.yml.j2 collections: - redhat.eap roles: - eap_install - eap_systemd - eap_validation &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's execute this playbook again:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory -e rhn_username=&lt;client_id&gt; -e rhn_password=&lt;client_secret&gt; eap_jms.yml PLAY [Deploy a Red Hat JBoss EAP server and set up a JMS Queue] **************** TASK [Gathering Facts] ********************************************************* ok: [localhost] TASK [redhat.eap.eap_install : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_install : Ensure prerequirements are fullfilled.] ********* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/prereqs.yml for localhost TASK [redhat.eap.eap_install : Validate credentials] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Validate existing zipfiles for offline installs] *** skipping: [localhost] TASK [redhat.eap.eap_install : Check that required packages list has been provided.] *** ok: [localhost] TASK [redhat.eap.eap_install : Prepare packages list] ************************** skipping: [localhost] TASK [redhat.eap.eap_install : Add JDK package java-11-openjdk-headless to packages list] *** ok: [localhost] TASK [redhat.eap.eap_install : Install required packages (4)] ****************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required local user exists.] ************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/user.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_install : Ensure workdir /opt/jboss_eap/ exists.] ********* ok: [localhost] TASK [redhat.eap.eap_install : Ensure archive_dir /opt/jboss_eap/ exists.] ***** ok: [localhost] TASK [redhat.eap.eap_install : Ensure server is installed] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_install/tasks/install.yml for localhost TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Check local download archive path] ************** ok: [localhost] TASK [redhat.eap.eap_install : Set download paths] ***************************** ok: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from website: https://github.com/eap/eap/releases/download] *** skipping: [localhost] TASK [redhat.eap.eap_install : Retrieve archive from RHN] ********************** skipping: [localhost] TASK [redhat.eap.eap_install : Install server using RPM] *********************** skipping: [localhost] TASK [redhat.eap.eap_install : Check downloaded archive] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Copy archive to target nodes] ******************* skipping: [localhost] TASK [redhat.eap.eap_install : Check target archive: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Verify target archive state: /opt/jboss_eap//jboss-eap-7.4.0.zip] *** ok: [localhost] TASK [redhat.eap.eap_install : Read target directory information: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Extract files from /opt/jboss_eap//jboss-eap-7.4.0.zip into /opt/jboss_eap/.] *** skipping: [localhost] TASK [redhat.eap.eap_install : Note: decompression was not executed] *********** ok: [localhost] =&gt; { "msg": "/opt/jboss_eap/jboss-eap-7.4/ already exists and version unchanged, skipping decompression" } TASK [redhat.eap.eap_install : Read information on server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Check state of server home directory: /opt/jboss_eap/jboss-eap-7.4/] *** ok: [localhost] TASK [redhat.eap.eap_install : Set instance name] ****************************** ok: [localhost] TASK [redhat.eap.eap_install : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_install : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for cumulative patch application are provided.] *** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [Apply latest cumulative patch] ******************************************* TASK [redhat.eap.eap_utils : Check installation] ******************************* ok: [localhost] TASK [redhat.eap.eap_utils : Set patch directory] ****************************** ok: [localhost] TASK [redhat.eap.eap_utils : Set download patch archive path] ****************** ok: [localhost] TASK [redhat.eap.eap_utils : Set patch destination directory] ****************** ok: [localhost] TASK [redhat.eap.eap_utils : Check download patch archive path] **************** ok: [localhost] TASK [redhat.eap.eap_utils : Check local download archive path] **************** ok: [localhost] TASK [redhat.eap.eap_utils : Check local downloaded archive: jboss-eap-7.4.9-patch.zip] *** ok: [localhost] TASK [redhat.eap.eap_utils : Retrieve product download using JBossNetwork API] *** skipping: [localhost] TASK [redhat.eap.eap_utils : Determine patch versions list] ******************** skipping: [localhost] TASK [redhat.eap.eap_utils : Determine latest version] ************************* skipping: [localhost] TASK [redhat.eap.eap_utils : Determine install zipfile from search results] **** skipping: [localhost] TASK [redhat.eap.eap_utils : Determine selected patch from supplied version: 7.4.9] *** skipping: [localhost] TASK [redhat.eap.eap_utils : Check remote downloaded archive: /opt/jboss-eap-7.4.9-patch.zip] *** skipping: [localhost] TASK [redhat.eap.eap_utils : Download Red Hat EAP patch] *********************** skipping: [localhost] TASK [redhat.eap.eap_utils : Set download patch archive path] ****************** ok: [localhost] TASK [redhat.eap.eap_utils : Check remote download patch archive path] ********* ok: [localhost] TASK [redhat.eap.eap_utils : Copy patch archive to target nodes] *************** changed: [localhost] TASK [redhat.eap.eap_utils : Check patch state] ******************************** ok: [localhost] TASK [redhat.eap.eap_utils : Set checksum file path for patch] ***************** ok: [localhost] TASK [redhat.eap.eap_utils : Check /opt/jboss_eap/jboss-eap-7.4//.applied_patch_checksum_f641b6de2807fac18d2a56de7a27c1ea3611e5f3.txt state] *** ok: [localhost] TASK [redhat.eap.eap_utils : Print when patch has been applied already] ******** skipping: [localhost] TASK [redhat.eap.eap_utils : Check if management interface is reachable] ******* ok: [localhost] TASK [redhat.eap.eap_utils : Set apply CP conflict default strategy to default (if not defined): --override-all] *** ok: [localhost] TASK [redhat.eap.eap_utils : Apply patch /opt/jboss-eap-7.4.9-patch.zip to server installed in /opt/jboss_eap/jboss-eap-7.4/] *** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_utils/tasks/jboss_cli.yml for localhost TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query 'patch apply --override-all /opt/jboss-eap-7.4.9-patch.zip'] *** ok: [localhost] TASK [redhat.eap.eap_utils : Display patching result] ************************** ok: [localhost] =&gt; { "msg": "Apply patch operation result: {\n \"outcome\" : \"success\",\n \"response-headers\" : {\n \"operation-requires-restart\" : true,\n \"process-state\" : \"restart-required\"\n }\n}" } TASK [redhat.eap.eap_utils : Set checksum file] ******************************** changed: [localhost] TASK [redhat.eap.eap_utils : Set latest patch file] **************************** changed: [localhost] TASK [redhat.eap.eap_utils : Restart server to ensure patch content is running] *** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_utils/tasks/jboss_cli.yml for localhost TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query 'shutdown --restart'] *********** ok: [localhost] TASK [redhat.eap.eap_utils : Wait for management interface is reachable] ******* ok: [localhost] TASK [redhat.eap.eap_utils : Stop service if it was started for patching] ****** skipping: [localhost] TASK [redhat.eap.eap_utils : Display resulting output] ************************* skipping: [localhost] TASK [redhat.eap.eap_install : Ensure required parameters for elytron adapter are provided.] *** skipping: [localhost] TASK [Install elytron adapter] ************************************************* skipping: [localhost] TASK [redhat.eap.eap_install : Install server using Prospero] ****************** skipping: [localhost] TASK [redhat.eap.eap_install : Check eap install directory state] ************** ok: [localhost] TASK [redhat.eap.eap_install : Validate conditions] **************************** ok: [localhost] TASK [Ensure firewalld configuration allows server port (if enabled).] ********* skipping: [localhost] TASK [redhat.eap.eap_systemd : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check current EAP patch installed] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments for yaml configuration] ********* ok: [localhost] TASK [Ensure required local user and group exists.] **************************** TASK [redhat.eap.eap_install : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_install : Set eap group] ********************************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure group eap exists.] *********************** ok: [localhost] TASK [redhat.eap.eap_install : Ensure user eap exists.] ************************ ok: [localhost] TASK [redhat.eap.eap_systemd : Set destination directory for configuration] **** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance destination directory for configuration] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set base directory for instance] **************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Check arguments] ******************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set instance name] ****************************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set bind address] ******************************* ok: [localhost] TASK [redhat.eap.eap_systemd : Create basedir /opt/jboss_eap/jboss-eap-7.4//standalone for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Create deployment directories for instance: eap] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy custom configuration] ******************** skipping: [localhost] TASK [redhat.eap.eap_systemd : Deploy configuration] *************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Include YAML configuration extension] *********** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/yml_config.yml for localhost TASK [redhat.eap.eap_systemd : Create YAML configuration directory] ************ skipping: [localhost] TASK [redhat.eap.eap_systemd : Enable YAML configuration extension] ************ skipping: [localhost] TASK [redhat.eap.eap_systemd : Create YAML configuration directory] ************ changed: [localhost] TASK [redhat.eap.eap_systemd : Enable YAML configuration extension] ************ changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy YAML configuration files] **************** changed: [localhost] =&gt; (item=jms_configuration.yml.j2) TASK [redhat.eap.eap_systemd : Check YAML configuration is disabled] *********** skipping: [localhost] TASK [redhat.eap.eap_systemd : Set systemd envfile destination] **************** ok: [localhost] TASK [redhat.eap.eap_systemd : Determine JAVA_HOME for selected JVM RPM] ******* ok: [localhost] TASK [redhat.eap.eap_systemd : Set systemd unit file destination] ************** ok: [localhost] TASK [redhat.eap.eap_systemd : Deploy service instance configuration: /etc//eap.conf] *** changed: [localhost] TASK [redhat.eap.eap_systemd : Deploy Systemd configuration for service: /usr/lib/systemd/system/eap.service] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Perform daemon-reload to ensure the changes are picked up] *** ok: [localhost] TASK [redhat.eap.eap_systemd : Ensure service is started] ********************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost TASK [redhat.eap.eap_systemd : Check arguments] ******************************** ok: [localhost] TASK [redhat.eap.eap_systemd : Set instance eap state to started] ************** ok: [localhost] TASK [redhat.eap.eap_validation : Validating arguments against arg spec 'main'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of user: eap] ***************** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure user eap were created.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Validate state of group: eap.] *************** ok: [localhost] TASK [redhat.eap.eap_validation : Wait for HTTP port 8080 to become available.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Check if web connector is accessible] ******** ok: [localhost] TASK [redhat.eap.eap_validation : Populate service facts] ********************** ok: [localhost] TASK [redhat.eap.eap_validation : Check if service is running] ***************** ok: [localhost] =&gt; { "changed": false, "msg": "All assertions passed" } TASK [redhat.eap.eap_validation : Verify server's internal configuration] ****** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/core-service=server-environment:read-attribute(name=start-gracefully)'}) included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/verify_with_cli_queries.yml for localhost =&gt; (item={'query': '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'}) TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /core-service=server-environment:read-attribute(name=start-gracefully)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/core-service=server-environment:read-attribute(name=start-gracefully)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure required parameters are provided.] **** ok: [localhost] TASK [Use CLI query to validate service state: /subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)] *** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query '/subsystem=undertow/server=default-server/http-listener=default:read-attribute(name=enabled)'] *** ok: [localhost] TASK [redhat.eap.eap_validation : Validate CLI query was successful] *********** ok: [localhost] TASK [redhat.eap.eap_validation : Transform output to JSON] ******************** ok: [localhost] TASK [redhat.eap.eap_validation : Display transformed result] ****************** skipping: [localhost] TASK [redhat.eap.eap_validation : Check that query was successfully performed.] *** ok: [localhost] TASK [redhat.eap.eap_validation : Ensure yaml setup] *************************** included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_validation/tasks/yaml_setup.yml for localhost TASK [Check standard-sockets configuration settings] *************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=mail-smtp:read-attribute(name=host)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of standard-sockets configuration settings] *** ok: [localhost] TASK [Check ejb configuration settings] **************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ejb3:read-attribute(name=default-resource-adapter-name)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ejb configuration settings] *** ok: [localhost] TASK [Check ee configuration settings] ***************************************** TASK [redhat.eap.eap_utils : Ensure required params for JBoss CLI have been provided] *** ok: [localhost] TASK [redhat.eap.eap_utils : Ensure server's management interface is reachable] *** ok: [localhost] TASK [redhat.eap.eap_utils : Execute CLI query /subsystem=ee/service=default-bindings:read-attribute(name=jms-connection-factory)] *** ok: [localhost] TASK [redhat.eap.eap_validation : Display result of ee configuration settings] *** ok: [localhost] RUNNING HANDLER [redhat.eap.eap_systemd : Restart Wildfly] ********************* included: /root/.ansible/collections/ansible_collections/redhat/eap/roles/eap_systemd/tasks/service.yml for localhost RUNNING HANDLER [redhat.eap.eap_systemd : Check arguments] ********************* ok: [localhost] RUNNING HANDLER [redhat.eap.eap_systemd : Set instance eap state to restarted] *** changed: [localhost] PLAY RECAP ********************************************************************* localhost : ok=127 changed=8 unreachable=0 failed=0 skipped=34 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As illustrated in the output above, the YAML definition is now enabled and the configuration of the JBoss EAP running on the target host has been updated.&lt;/p&gt; &lt;h3&gt;3.3 Validate the JMS queue deployment&lt;/h3&gt; &lt;p&gt;As always, we are going to be thorough and verify that the playbook execution has, indeed, properly set up a JMS queue. To do so, we can simply use the JBoss CLI provided with JBoss EAP to confirm:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ /opt/jboss_eap/jboss-eap-7.4/bin/jboss-cli.sh --connect --command="/subsystem=messaging-activemq/server=default/jms-queue=MyQueue:read-resource" { "outcome" =&gt; "success", "result" =&gt; { "durable" =&gt; true, "entries" =&gt; ["queues/MyQueue"], "legacy-entries" =&gt; undefined, "selector" =&gt; undefined } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output, as shown above, confirms that the server configuration has indeed been updated and that a brand new JMS queue is now available. Since this verification is fairly easy to automate, we will also add it to our playbook.&lt;/p&gt; &lt;p&gt;The Ansible collection for JBoss EAP comes with a handy wrapper allowing for the execution of the JBoss CLI within a playbook. So, all that is needed is the inclusion of the task and the desired command, as shown below:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;post_tasks: - name: "Check that Queue {{ queue.name }} is available." ansible.builtin.include_role: name: eap_utils tasks_from: jboss_cli.yml vars: jboss_home: "{{ eap_home }}" jboss_cli_query: "/subsystem=messaging-activemq/server=default/jms-queue={{ queue.name }}:read-resource"&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Thanks to the Ansible collection for JBoss EAP, we have a minimalistic playbook, spared of all the heavy lifting of managing the Java application server, fulfilling the role of MOM. All the configuration required by the automation concerns only the use case we tried to implement, not the inner working of the solution (JBoss EAP). All the configuration required by the automation concerns only the use case we tried to implement, not the inner working of the solution (JBoss EAP). The resulting playbook is safely repeatable and can be used to install the software on any number of target systems. Using the collection for JBoss EAP also makes it easy to keep the deployment up to date.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/01/automate-message-queue-deployment-jboss-eap" title="Automate message queue deployment on JBoss EAP"&gt;Automate message queue deployment on JBoss EAP&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2023-09-01T07:00:00Z</dc:date></entry><entry><title>How we ensure statically linked applications stay that way</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/31/how-we-ensure-statically-linked-applications-stay-way" /><author><name>Arjun Shankar</name></author><id>f47b49a2-c3b2-480f-ad54-b943d9caebdf</id><updated>2023-08-31T07:00:00Z</updated><published>2023-08-31T07:00:00Z</published><summary type="html">&lt;p&gt;While glibc's highly configurable name resolution and character set handling features offer an advantage when it comes to system configuration and installed content, there are limitations when it comes to statically linked applications. This article summarizes the current state, recent improvements, and plans for moving toward truly statically linked applications.&lt;/p&gt; &lt;p&gt;Although dynamic linking has advantages, making it the default choice for situations where binary compatibility is guaranteed (i.e., many &lt;a href="https://access.redhat.com/articles/rhel9-abi-compatibility"&gt;Red Hat Enterprise Linux&lt;/a&gt; components), static linking is still useful in many situations such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Application developers wanting to ensure that their binaries run on a wide range of operating system vendors and versions.&lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Container introspection tools injected via oc-inject might bring their own dependencies into the system via static linking to avoid depending on in-container libraries.&lt;/li&gt; &lt;li aria-level="1"&gt;Languages such as Go might produce statically linked binaries by default.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Statically linking against glibc&lt;/h2&gt; &lt;p&gt;Regardless of the use case, when you statically link an application, the reasonable expectation is that all dependencies are linked statically. When it comes to glibc, this is currently not the case due to several glibc features, such as:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Name service switch (nsswitch)&lt;/li&gt; &lt;li aria-level="1"&gt;Character set handling and conversion (iconv)&lt;/li&gt; &lt;li aria-level="1"&gt;Thread cancellation (unwinder)&lt;/li&gt; &lt;li aria-level="1"&gt;Internationalized domain names (libidn2)&lt;/li&gt; &lt;li aria-level="1"&gt;Dynamic library loading from static code (dlopen)&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The first two features allow glibc to be highly configurable when it comes to name resolution and character set handling. There are several non-glibc name resolution services implemented in the form of NSS modules. For example, systemd provides a caching DNS resolver installed by default on Fedora. Along the same lines, while vendor plugins for character set handling are less common, glibc's fairly exhaustive list of character set converters can be shipped and installed separately, reducing system footprint.&lt;/p&gt; &lt;p&gt;These features also come with a limitation related to statically linked applications. The application can potentially dynamically load (dlopen) NSS or iconv modules during execution, depending on system configuration and module availability. A documented fact which still goes against the principle of least surprise. In the case of Go, while it tries to generate statically linked binaries by default, linking against glibc for NSS ends up making &lt;a data-sk="tooltip_parent" data-stringify-link="https://mt165.co.uk/blog/static-link-go/" href="https://mt165.co.uk/blog/static-link-go/" target="_blank"&gt;"many Go programmes dynamically-linked, to the point that a lot of people think Go dynamically links by default / preference&lt;/a&gt;."&lt;/p&gt; &lt;h2&gt;Recent changes&lt;/h2&gt; &lt;p&gt;The GNU C library offers an --enable-static-nss configure time option that builds libc.a (i.e., statically linked glibc) in a way that NSS modules are statically linked. However, a relatively recent refactor of NSS code removed this feature. It was never enabled for Fedora’s or RHEL’s glibc-static package. Since the refactor, the DNS and files back-ends (the most frequently used) have been moved into glibc instead of being shipped as separate modules. This means that doing name lookups from /etc/hosts or /etc/passwd does not lead to an NSS module load as long as /etc/nsswitch.conf only lists the files and DNS database providers. This has left the --enable-static-nss configure option redundant. However, there is more to be done here.&lt;/p&gt; &lt;h2&gt;Iterating toward a solution&lt;/h2&gt; &lt;p&gt;While there are five subsystems in this problem area, I have picked two to begin the process of iterating toward a solution: NSS and iconv.&lt;/p&gt; &lt;p&gt;The next step toward avoiding surprising NSS module loads in statically linked applications is to either bring back the functionality that --enable-static-nss provides or do even better by allowing configuring behavior at application build-time instead of distribution build-time and providing a way to completely suppress all dynamic loading, regardless of the contents of nsswitch.conf requiring it.&lt;/p&gt; &lt;p&gt;I'm currently in the process of trying to solve this. I did some initial experimentation by trying to build a new statically linked library that contains copies of glibc internal functions involved in NSS module loading, but with the module loading disabled. The idea being that with the appropriate linker command line, this can be used to link the alternative (non module loading) versions of the NSS component instead of the default versions that call dlopen. Once I had an idea of the changes we are looking at, I &lt;a href="https://sourceware.org/pipermail/libc-alpha/2023-May/148682.html"&gt;started a conversation upstream&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I am now working through the implementation details, starting with collecting NSS code scattered across the glibc source tree into one place. Once I am finished with that, I will start posting patches upstream and working toward consensus.&lt;/p&gt; &lt;p&gt;Please stay tuned for my future article describing glibc's progress on supporting truly statically linked applications.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/31/how-we-ensure-statically-linked-applications-stay-way" title="How we ensure statically linked applications stay that way"&gt;How we ensure statically linked applications stay that way&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Arjun Shankar</dc:creator><dc:date>2023-08-31T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.43.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/08/kogito-1-43-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/08/kogito-1-43-0-released.html</id><updated>2023-08-30T07:33:24Z</updated><content type="html">We are glad to announce that the Kogito 1.43.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Track user identity in Kogito events * Allow PostgreSQL persistence for process instance state when using SW executor * New endpoints REST to retrieve workflows and schema For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.31.0 artifacts are available at the . A detailed changelog for 1.43.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry></feed>
